{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2d1d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pyra.dst import handle_dst_dates\n",
    "from pyra.loadagg import average_previous_and_subsequent_dates, average_previous_and_subsequent_hours\n",
    "from pyra.date_utils import daily_index, hourly_index, planning_year\n",
    "import shutil\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import win32com.client\n",
    "import pythoncom\n",
    "\n",
    "# Scraping libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c42a01",
   "metadata": {},
   "source": [
    "## EMTDB Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06dc1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(r'K:\\Valuation\\_Analysts\\Hemanth\\Python Notebooks\\Miscellaneous\\Python Analyst Engine 2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af5aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import EmtdbConnection\n",
    "from emtdb_api import pull_lmp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ab2df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'HXH07BP'\n",
    "pw = getpass('Enter EMTDB pass:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a404c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "emtdb = EmtdbConnection(user, pw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eac47e",
   "metadata": {},
   "source": [
    "## Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15fdd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up session (some sites require headers)\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "})\n",
    "\n",
    "# Get the page\n",
    "url = 'https://bgs-auction.com/bgs.dataroom.asp' # monthly data page\n",
    "# url = 'https://bgs-auction.com/bgs.dataroom.occ.asp' # additional data page\n",
    "# url = 'https://www.bgs-auction.com/bgs.auction.regproc.asp#sec2' # Documents page\n",
    "response = session.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all links that point to files (adjust selectors as needed)\n",
    "file_links = []\n",
    "for link in soup.find_all('a', href=True):\n",
    "    href = link['href']\n",
    "    # Look for common file extensions\n",
    "    extensions = [\n",
    "        # '.pdf',\n",
    "        # '.xlsx',\n",
    "        # '.xls',\n",
    "        '.xlsb',\n",
    "        # '.csv',\n",
    "        # '.zip',\n",
    "        # '.doc',\n",
    "        # '.docx'\n",
    "    ]\n",
    "    if any(href.lower().endswith(ext) for ext in extensions):\n",
    "        full_url = urljoin(url, href)\n",
    "        file_links.append(full_url)\n",
    "\n",
    "# Create download directory\n",
    "download_dir = Path('bgs_files')\n",
    "download_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download each file\n",
    "for file_url in file_links:\n",
    "    filename = Path(file_url).name\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    \n",
    "    try:\n",
    "        file_response = session.get(file_url, timeout=30)\n",
    "        file_response.raise_for_status()\n",
    "        \n",
    "        filepath = download_dir / filename\n",
    "        with open(filepath, 'wb') as f:\n",
    "            f.write(file_response.content)\n",
    "        print(f\"✓ Saved {filename}\")\n",
    "        \n",
    "        time.sleep(0.5)  # Be polite to the server\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to download {filename}: {e}\")\n",
    "\n",
    "print(f\"\\nDownloaded {len(file_links)} files to {download_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f980d73",
   "metadata": {},
   "source": [
    "Unzipping files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981484bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder containing zip files\n",
    "zip_folder = Path('bgs_files')  # or wherever your zips are\n",
    "\n",
    "# Find and extract all zip files\n",
    "for zip_path in zip_folder.glob('*.zip'):\n",
    "    print(f\"Extracting {zip_path.name}...\")\n",
    "    \n",
    "    # Create extraction folder (same name as zip file)\n",
    "    extract_to = zip_folder / zip_path.stem\n",
    "    extract_to.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Extract\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    \n",
    "    print(f\"✓ Extracted to {extract_to}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78016d47",
   "metadata": {},
   "source": [
    "## Data Aggregation and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710e016d",
   "metadata": {},
   "source": [
    "### Setting up file paths and helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49de6049",
   "metadata": {},
   "source": [
    "Changes made to aid aggregation:\n",
    "1. Rename the string 'PSE&G' in newer file names to 'PSEG' to match old file naming convention\n",
    "2. Replace '_' in file_name by '-' to match old file naming convention\n",
    "3. Replace Feburary with February in Hist_CIEP_Switching_Statistics.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c99a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main folder paths\n",
    "\n",
    "zones = ['ACE', 'JCP&L', 'JCP&L_Excess_Generation', 'PSEG', 'RECO']\n",
    "zones_without_gen = ['ACE', 'JCP&L', 'PSEG', 'RECO']\n",
    "\n",
    "# The below folder has information that is updated less frequently\n",
    "less_frequent_data_folder_path = r\"K:\\Valuation\\Wholesale for Retail\\_Full Requirements\\PJM\\BGS FP & CIEP\\2026-02\\CIEP\\Pri Hemanth\\Data\\bgs_files_less_frequent\"\n",
    "\n",
    "# The below folder has information that is updated on a monthly basis\n",
    "monthly_data_folder_path = r\"K:\\Valuation\\Wholesale for Retail\\_Full Requirements\\PJM\\BGS FP & CIEP\\2026-02\\CIEP\\Pri Hemanth\\Data\\bgs_files_monthly\" \n",
    "\n",
    "# Historical data folder names\n",
    "\n",
    "ace_historical_folder_name = 'ACE_Historical_Data_(Prior_to_June_2022)'\n",
    "jcpl_historical_folder_name = 'JCP&L_Historical_Data_(Prior_to_June_2022)'\n",
    "pseg_historical_folder_name = 'PSE&G_Historical_Data_(Prior_to_June_2022)'\n",
    "reco_historical_folder_name = 'RECO_Historical_Data_(Prior_to_June_2022)'\n",
    "recent_folder_name = 'CIEP_Recent_Data_(After_June_2022)'\n",
    "switching_folder_name = 'Switching_Statistics_(Prior_to_June_2022)'\n",
    "\n",
    "folder_names = [\n",
    "    ace_historical_folder_name,\n",
    "    jcpl_historical_folder_name,\n",
    "    pseg_historical_folder_name,\n",
    "    reco_historical_folder_name,\n",
    "    recent_folder_name\n",
    "]\n",
    "\n",
    "folder_names_without_recent = folder_names[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def flatten_columns(DF, sep='_'):\n",
    "    temp_DF = DF.copy()\n",
    "    temp_DF.columns = [sep.join(map(str, col)).strip(sep) for col in temp_DF.columns]\n",
    "    return temp_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38944418",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447919fe",
   "metadata": {},
   "source": [
    "#### Helper functions to aggregate load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67ee4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to aggregate load\n",
    "\n",
    "def aggregate_load(folder_path, file_name, file_type):\n",
    "    # Reads load from given file_path and returns DataFrame. We have slightly different code for csv and excel since the file names and formats are slightly different.\n",
    "\n",
    "    print(f'Aggregating load data from {file_name}')\n",
    "\n",
    "    if file_type == 'csv':\n",
    "        \n",
    "        if 'Generation' not in file_name:\n",
    "\n",
    "            if 'Eligible' not in file_name:\n",
    "                Load_Name = file_name.split('_')[1] + '_' + file_name.split('_')[2].split('-')[0]\n",
    "            else:\n",
    "                Load_Name = file_name.split('_')[1] + '_' + file_name.split('_')[2].split('-')[1]\n",
    "\n",
    "        else: # If the file_name contains the word 'Generation' - for JCP&L\n",
    "\n",
    "            if 'Eligible' not in file_name:\n",
    "                Load_Name = file_name.split('_')[1] + '_' + file_name.split('_')[5] + '_' + file_name.split('_')[6][:-4] + '_' + file_name.split('_')[2] # To remove the .csv string at the end\n",
    "            else:\n",
    "                Load_Name = file_name.split('_')[1] + '_' + file_name.split('_')[5] + '_' + file_name.split('_')[6][:-4]  + '_' + file_name.split('_')[3] # To remove the .csv string at the end\n",
    "\n",
    "        return pd.read_csv(\n",
    "                    os.path.join(folder_path, file_name),\n",
    "                    skiprows=4\n",
    "                    ).melt(\n",
    "                    id_vars='Unnamed: 0'\n",
    "                ).rename(\n",
    "                    columns={\n",
    "                        'Unnamed: 0': 'Date',\n",
    "                        'variable': 'Hour',\n",
    "                        'value': 'Load',\n",
    "                    }\n",
    "                ).assign(\n",
    "                    Hour=lambda DF: DF.Hour.str[-2:].astype(int),\n",
    "                    Date=lambda DF: pd.to_datetime(DF.Date),\n",
    "                    Load_Name=Load_Name\n",
    "                ).sort_values(\n",
    "                    by=['Date', 'Hour']\n",
    "                )\n",
    "    \n",
    "    elif file_type == 'excel':\n",
    "        \n",
    "        if 'Generation' not in file_name:\n",
    "\n",
    "            if 'Eligible' not in file_name:\n",
    "                Load_Name = file_name.split('_')[0] + '_' + file_name.split('_')[1].split('-')[0]\n",
    "            else:\n",
    "                Load_Name = file_name.split('_')[0] + '_' + file_name.split('_')[1].split('-')[1]\n",
    "\n",
    "        else: # If the file_name contains the word 'Generation' - for JCP&L\n",
    "\n",
    "            if 'Eligible' not in file_name:\n",
    "                Load_Name = file_name.split('_')[0] + '_' + file_name.split('_')[4] + '_' + file_name.split('_')[5] + '_' + file_name.split('_')[1]\n",
    "            else:\n",
    "                Load_Name = file_name.split('_')[0] + '_' + file_name.split('_')[3] + '_' + file_name.split('_')[4] + '_' + file_name.split('_')[1].split('-')[1]    \n",
    "\n",
    "        if 'xlsx' in file_name: # For usual excel files\n",
    "            engine=None\n",
    "        elif 'xlsb' in file_name:\n",
    "            engine='pyxlsb'\n",
    "\n",
    "        return pd.read_excel(\n",
    "                    os.path.join(folder_path, file_name),\n",
    "                    engine=engine,\n",
    "                    skiprows=5 # The format of the excel files are shifted by a row compared to the csv files\n",
    "                    ).melt(\n",
    "                    id_vars='Unnamed: 0'\n",
    "                ).rename(\n",
    "                    columns={\n",
    "                        'Unnamed: 0': 'Date',\n",
    "                        'variable': 'Hour',\n",
    "                        'value': 'Load',\n",
    "                    }\n",
    "                ).assign(\n",
    "                    Hour=lambda DF: DF.Hour.str[-2:].astype(int),\n",
    "                    # Date=lambda DF: pd.to_datetime(DF.Date),\n",
    "                    Date=lambda DF: pd.to_datetime('1899-12-30') + pd.to_timedelta(DF.Date, unit='D') if 'xlsb' in file_name else pd.to_datetime(DF.Date), # Adding this since pandas messes up reading dates from xlsb files sometimes\n",
    "                    Load_Name=Load_Name\n",
    "                ).sort_values(\n",
    "                    by=['Date', 'Hour']\n",
    "                )             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af77878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load = pd.DataFrame()\n",
    "\n",
    "for folder_name in folder_names:\n",
    "    \n",
    "    folder_path = os.path.join(monthly_data_folder_path, folder_name, 'CIEP') # Only aggregating CIEP data\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "\n",
    "        if ('Load' in file_name) or ('Generation' in file_name): # Adding this filter only to aggregate load data first. 'Generation' is also included for JCP&L\n",
    "            \n",
    "            if 'Recent' in folder_name:\n",
    "                df_load_temp = aggregate_load(folder_path, file_name, 'excel') # Newer files in excel\n",
    "            else:\n",
    "                df_load_temp = aggregate_load(folder_path, file_name, 'csv') # Older files in csv\n",
    "        \n",
    "            df_load = pd.concat([df_load, df_load_temp])\n",
    "\n",
    "df_load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432fbf22",
   "metadata": {},
   "source": [
    "#### Doing some checks before aggregating load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae4c0df",
   "metadata": {},
   "source": [
    "Observed a number of NaNs for Hour 2, 3 and 25 and some ' -   ' for the following 2 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5d0991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hours with missing load\n",
    "\n",
    "df_load[df_load.Load.isna()].Hour.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453750b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows with non-numeric load\n",
    "\n",
    "df_load.dropna()[pd.to_numeric(df_load.dropna()['Load'], errors='coerce').isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661d2a2a",
   "metadata": {},
   "source": [
    "#### Pivoting load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eaaf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load_pivoted = df_load.dropna().replace( # Dropping NaNs which correspond to hours 2, 3 and 25 and replacing the weird string with NaN\n",
    "    ' -   ', np.nan \n",
    ").assign(\n",
    "    Load=lambda DF: DF.Load.astype(float)\n",
    ").pipe(\n",
    "    lambda DF: DF[DF.Hour != 25] # Getting rid of Hour 25\n",
    ").pivot(\n",
    "    index=['Date', 'Hour'],\n",
    "    columns='Load_Name',\n",
    "    values='Load'\n",
    ").sort_index(\n",
    "    level=[\n",
    "        'Date', 'Hour'\n",
    "    ]\n",
    ").reset_index().pipe(\n",
    "    lambda DF: DF[DF.Date >= '2014-01-01'] # Filtering out data before 2014. Raw data starts on 2001-08-01.\n",
    ").set_index(\n",
    "    ['Date', 'Hour']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eb6303",
   "metadata": {},
   "source": [
    "#### Load adjustments - very tedious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the following adjustments before DST adjustments since the hour 2 (and sometimes 4) seems to have been doubled on these dates right before the DST \n",
    "\n",
    "for date in ['2016-03-13', '2020-03-08', '2024-03-10']:\n",
    "    df_load_pivoted['PSEG_BGS'].loc[\n",
    "    pd.IndexSlice[date, 2]\n",
    "] = df_load_pivoted['PSEG_BGS'].loc[\n",
    "    pd.IndexSlice[date, 2]\n",
    "] / 2\n",
    "\n",
    "    df_load_pivoted['PSEG_Eligible'].loc[\n",
    "    pd.IndexSlice[date, 2]\n",
    "] = df_load_pivoted['PSEG_Eligible'].loc[\n",
    "    pd.IndexSlice[date, 2]\n",
    "] / 2\n",
    "\n",
    "\n",
    "df_load_pivoted['PSEG_BGS'].loc[\n",
    "    pd.IndexSlice['2022-03-13', 4]\n",
    "] = df_load_pivoted['PSEG_BGS'].loc[\n",
    "    pd.IndexSlice['2022-03-13', 4]\n",
    "] / 2\n",
    "\n",
    "df_load_pivoted['PSEG_Eligible'].loc[\n",
    "    pd.IndexSlice['2022-03-13', 4]\n",
    "] = df_load_pivoted['PSEG_Eligible'].loc[\n",
    "    pd.IndexSlice['2022-03-13', 4]\n",
    "] / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4331c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing existing values with NaN so that they can be interpolated. Note that only the specific range is intepolated since the other NaNs will be handled separately by \n",
    "# the DST function\n",
    "\n",
    "df_load_pivoted['PSEG_Eligible'].loc[\n",
    "    pd.IndexSlice['2020-09-19', 10:16]\n",
    "] = np.nan\n",
    "\n",
    "df_load_pivoted['PSEG_Eligible'].loc[\n",
    "    pd.IndexSlice['2020-09-20', 9:17]\n",
    "] = np.nan\n",
    "\n",
    "df_load_pivoted['PSEG_Eligible'].loc[\n",
    "    pd.IndexSlice['2020-09-19', 1:24]\n",
    "] = df_load_pivoted['PSEG_Eligible'].loc[\n",
    "    pd.IndexSlice['2020-09-19', 1:24]\n",
    "].interpolate()\n",
    "\n",
    "df_load_pivoted['PSEG_Eligible'].loc[\n",
    "    pd.IndexSlice['2020-09-20', 1:24]\n",
    "] = df_load_pivoted['PSEG_Eligible'].loc[\n",
    "    pd.IndexSlice['2020-09-20', 1:24]\n",
    "].interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feed129",
   "metadata": {},
   "source": [
    "Doing the DST adjustments zone by zone since the function handle_dst_dates cannot handle all of them at once if the missing hours are not the same across all the loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2505a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for zone in zones:\n",
    "    print(f'Doing DST adjustments for {zone} load \\n')\n",
    "    df_load_pivoted[[zone + '_BGS', zone + '_Eligible']] = df_load_pivoted[[zone + '_BGS', zone + '_Eligible']].pipe(\n",
    "        handle_dst_dates,\n",
    "        verbose=True\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df069023",
   "metadata": {},
   "source": [
    "Outliers found in data:\n",
    "\n",
    "1. ACE_BGS and ACE_Eligible has some outlier data points on 2016-04-28, 2017-10-18 and 2024-05-11 to 2024-05-13\n",
    "2. RECO_BGS has some drops from 2019-04-24 through 2019-04-30 and 2024-01-31\n",
    "3. RECO_Eligible have some drops from 2019-04-24 through 2019-04-30\n",
    "4. PSEG_Eligible has some high spikes on 2 dates which have been averaged by the previous and next hours\n",
    "5. RECO_BGS and RECO_Eligible have the DST ending hours doubled on certain years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac262620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original approach\n",
    "# df_load_pivoted['ACE_BGS'] = average_previous_and_subsequent_dates(\n",
    "#     df_load_pivoted['ACE_BGS'],\n",
    "#     date=pd.to_datetime('2016-04-28')\n",
    "# )\n",
    "\n",
    "# Holding flat to the previous day for ACE_BGS and ACE_Eligible on 2016-04-28\n",
    "\n",
    "df_load_pivoted['ACE_BGS'].loc[\n",
    "    pd.IndexSlice['2016-04-28', 1:24]\n",
    "] = df_load_pivoted['ACE_BGS'].loc[\n",
    "    pd.IndexSlice['2016-04-27', 1:24]\n",
    "].to_numpy()\n",
    "\n",
    "df_load_pivoted['ACE_Eligible'].loc[\n",
    "    pd.IndexSlice['2016-04-28', 1:24]\n",
    "] = df_load_pivoted['ACE_Eligible'].loc[\n",
    "    pd.IndexSlice['2016-04-27', 1:24]\n",
    "].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Original approach\n",
    "# df_load_pivoted['ACE_BGS'] = average_previous_and_subsequent_dates(\n",
    "#     df_load_pivoted['ACE_BGS'],\n",
    "#     date=pd.to_datetime('2017-10-18')\n",
    "# )\n",
    "\n",
    "# Holding flat to the previous day for ACE_BGS and ACE_Eligible on 2017-10-18\n",
    "\n",
    "df_load_pivoted['ACE_BGS'].loc[\n",
    "    pd.IndexSlice['2017-10-18', 1:24]\n",
    "] = df_load_pivoted['ACE_BGS'].loc[\n",
    "    pd.IndexSlice['2017-10-17', 1:24]\n",
    "].to_numpy()\n",
    "\n",
    "df_load_pivoted['ACE_Eligible'].loc[\n",
    "    pd.IndexSlice['2017-10-18', 1:24]\n",
    "] = df_load_pivoted['ACE_Eligible'].loc[\n",
    "    pd.IndexSlice['2017-10-17', 1:24]\n",
    "].to_numpy()\n",
    "\n",
    "\n",
    "# Holding flat to the previous week for ACE_eligible between 2024-05-11 and 2024-05-13\n",
    "for date in pd.date_range('2024-05-11', '2024-05-13'):\n",
    "    df_load_pivoted['ACE_Eligible'].loc[\n",
    "    pd.IndexSlice[date, 1:24]\n",
    "] = df_load_pivoted['ACE_Eligible'].loc[\n",
    "    pd.IndexSlice[date - pd.DateOffset(7), 1:24]\n",
    "].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Original approach\n",
    "# df_load_pivoted['RECO_BGS'] = average_previous_and_subsequent_dates(\n",
    "#     df_load_pivoted['RECO_BGS'],\n",
    "#     date=pd.to_datetime('2024-01-31')\n",
    "# )\n",
    "\n",
    "# Replacing the data in hours 13 through 24 on 2019-04-24 with those from 2019-04-23 for RECO_BGS\n",
    "\n",
    "df_load_pivoted['RECO_BGS'].loc[\n",
    "    pd.IndexSlice['2019-04-24', 13:24]\n",
    "] = df_load_pivoted['RECO_BGS'].loc[\n",
    "    pd.IndexSlice['2019-04-23', 13:24]\n",
    "].to_numpy()\n",
    "\n",
    "\n",
    "# Replacing the data in hours 20 through 24 on 2024-04-31 with those from 2024-04-30 for RECO_BGS\n",
    "\n",
    "df_load_pivoted['RECO_BGS'].loc[\n",
    "    pd.IndexSlice['2024-01-31', 20:24]\n",
    "] = df_load_pivoted['RECO_BGS'].loc[\n",
    "    pd.IndexSlice['2024-01-30', 20:24]\n",
    "].to_numpy()\n",
    "\n",
    "\n",
    "# Replacing the data from 2019-04-25 through 2019-04-30 with the data from one week back for RECO_BGS\n",
    "for date in pd.date_range('2019-04-25', '2019-04-30'):\n",
    "    df_load_pivoted['RECO_BGS'].loc[\n",
    "        pd.IndexSlice[date, :]\n",
    "    ] = df_load_pivoted['RECO_BGS'].loc[\n",
    "    pd.IndexSlice[date - pd.DateOffset(7), :]\n",
    "].to_numpy()\n",
    "\n",
    "\n",
    "# Original approach\n",
    "# Replacing the data from 2019-04-25 through 2019-04-30 with the data on 2019-04-23 for RECO_BGS\n",
    "# for date in pd.date_range('2019-04-25', '2019-04-30'):\n",
    "#     df_load_pivoted['RECO_BGS'].loc[\n",
    "#         pd.IndexSlice[date, :]\n",
    "#     ] = df_load_pivoted['RECO_BGS'].loc[\n",
    "#     pd.IndexSlice['2019-04-23', :]\n",
    "# ].to_numpy()\n",
    "    \n",
    "\n",
    "# Replacing the data in hours 13 through 24 on 2019-04-24 with those from 2019-04-23 for RECO_Eligible\n",
    "\n",
    "\n",
    "df_load_pivoted['RECO_Eligible'].loc[\n",
    "    pd.IndexSlice['2019-04-24', 13:24]\n",
    "] = df_load_pivoted['RECO_Eligible'].loc[\n",
    "    pd.IndexSlice['2019-04-23', 13:24]\n",
    "].to_numpy()\n",
    "\n",
    "\n",
    "# Replacing the data from 2019-04-25 through 2019-04-30 with the data from one week back for RECO_Eligible\n",
    "for date in pd.date_range('2019-04-25', '2019-04-30'):\n",
    "    df_load_pivoted['RECO_Eligible'].loc[\n",
    "        pd.IndexSlice[date, :]\n",
    "    ] = df_load_pivoted['RECO_Eligible'].loc[\n",
    "    pd.IndexSlice[date - pd.DateOffset(7), :]\n",
    "].to_numpy()\n",
    "\n",
    "\n",
    "# Original approach\n",
    "\n",
    "# Replacing the data from 2019-04-25 through 2019-04-30 with the data on 2019-04-23 for RECO_Eligible\n",
    "\n",
    "# for date in pd.date_range('2019-04-25', '2019-04-30'):\n",
    "#     df_load_pivoted['RECO_Eligible'].loc[\n",
    "#         pd.IndexSlice[date, :]\n",
    "#     ] = df_load_pivoted['RECO_Eligible'].loc[\n",
    "#     pd.IndexSlice['2019-04-23', :]\n",
    "# ].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f0ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Halving the hour 2 for specific DST dates for RECO to match secondary - so stupid how much we care so much about \"tying\" on our desk\n",
    "\n",
    "reco_dst_halve_he_2_dates = [\n",
    "    '2015-11-01',\n",
    "    '2016-11-06',\n",
    "    '2017-11-05',\n",
    "    '2018-11-04',\n",
    "    '2019-11-03',\n",
    "    '2020-11-01',\n",
    "    '2021-11-07',\n",
    "    '2023-11-05'\n",
    "]\n",
    "\n",
    "for date in reco_dst_halve_he_2_dates:\n",
    "    df_load_pivoted['RECO_BGS'].loc[\n",
    "        pd.IndexSlice[date, 2]\n",
    "    ] = df_load_pivoted['RECO_BGS'].loc[\n",
    "    pd.IndexSlice[date, 2]\n",
    "] / 2\n",
    "    \n",
    "    df_load_pivoted['RECO_Eligible'].loc[\n",
    "        pd.IndexSlice[date, 2]\n",
    "    ] = df_load_pivoted['RECO_Eligible'].loc[\n",
    "    pd.IndexSlice[date, 2]\n",
    "] / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7923ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the outliers in the PSEG Eligible data\n",
    "\n",
    "# Original approach\n",
    "# df_load_pivoted[['PSEG_Eligible']] = average_previous_and_subsequent_hours(\n",
    "#     df_load_pivoted[['PSEG_Eligible']],\n",
    "#     pd.to_datetime('2016-03-13'),\n",
    "#     2\n",
    "# )\n",
    "\n",
    "# Original approach\n",
    "# df_load_pivoted[['PSEG_Eligible']] = average_previous_and_subsequent_hours(\n",
    "#     df_load_pivoted[['PSEG_Eligible']],\n",
    "#     pd.to_datetime('2020-03-08'),\n",
    "#     2\n",
    "# )\n",
    "\n",
    "# df_load_pivoted[['PSEG_Eligible']] = average_previous_and_subsequent_hours(\n",
    "#     df_load_pivoted[['PSEG_Eligible']],\n",
    "#     pd.to_datetime('2022-03-13'),\n",
    "#     4\n",
    "# )\n",
    "\n",
    "df_load_pivoted[['PSEG_Eligible']] = average_previous_and_subsequent_hours(\n",
    "    df_load_pivoted[['PSEG_Eligible']],\n",
    "    pd.to_datetime('2024-03-10'),\n",
    "    2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae6dee",
   "metadata": {},
   "source": [
    "#### Calculating JCP&L Excess Gen Load and plotting all the loads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e89c86e",
   "metadata": {},
   "source": [
    "Since excess generation is no longer part of the pricing load for JCP&L, we don't want to distort history by adding it. So even though I add it to below, we end up not using excess gen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c6963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load_pivoted = df_load_pivoted.assign(\n",
    "    JCPL_Net_BGS=lambda DF: (DF['JCP&L_BGS'] + DF['JCP&L_Excess_Generation_BGS']).where(DF.index.get_level_values(0) < '2022-08-01', DF['JCP&L_BGS']),\n",
    "    JCPL_Net_Eligible=lambda DF: (DF['JCP&L_Eligible'] + DF['JCP&L_Excess_Generation_Eligible']).where(DF.index.get_level_values(0) < '2022-08-01', DF['JCP&L_Eligible'])\n",
    ").rename(\n",
    "    columns={\n",
    "        'JCPL_Net_BGS': 'JCP&L_Net_BGS',\n",
    "        'JCPL_Net_Eligible': 'JCP&L_Net_Eligible'\n",
    "    }\n",
    ").reindex(\n",
    "    columns=[zone + '_BGS' for zone in zones] + ['JCP&L_Net_BGS'] + [zone + '_Eligible' for zone in zones] + ['JCP&L_Net_Eligible'] # following format requested by secondary \n",
    ")\n",
    "\n",
    "df_load_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc46c9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for zone in zones:\n",
    "    fig = df_load_pivoted.reset_index().pipe(\n",
    "        px.line,\n",
    "        x='Date',\n",
    "        # y=f'{zone}_BGS',\n",
    "        y=f'{zone}_BGS',\n",
    "        # y=[zone + '_BGS' for zone in zones],\n",
    "        # y=[zone + '_Eligible' for zone in zones],\n",
    "        title=f'{zone}_BGS',\n",
    "        labels={\n",
    "            'y': 'Load'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4630119",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load_pivoted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7480919",
   "metadata": {},
   "source": [
    "### Raw Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85027321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to aggregate tags\n",
    "\n",
    "def aggregate_tags(folder_path, file_name, file_type):\n",
    "    # Reads tags from given file_path and returns DataFrame. We have slightly different code for csv and excel since the file names and formats are slightly different.\n",
    "\n",
    "    if file_type == 'csv':\n",
    "        \n",
    "        if 'Generation' not in file_name:\n",
    "\n",
    "            if 'Eligible' not in file_name:\n",
    "                if 'Retail' not in file_name: # Adding this additional check since we're also aggregating RSCP and TOTAL tags for verification purposes\n",
    "                    Load_Name = file_name.split('_')[1] + '_' + file_name.split('_')[2].split('-')[0]\n",
    "                else:\n",
    "                    Load_Name = file_name.split('_')[1] + '_' + file_name.split('_')[3]\n",
    "            else:\n",
    "                Load_Name = file_name.split('_')[1] + '_' + file_name.split('_')[2].split('-')[1]\n",
    "\n",
    "        else: # If the file_name contains the word 'Generation' - for JCP&L\n",
    "            \n",
    "            if 'Eligible' not in file_name:\n",
    "                Load_Name = file_name.split('_')[1] + '_' + file_name.split('_')[2] + '_' + file_name.split('_')[5] + '_' + file_name.split('_')[6][:-4] # To remove the .csv string at the end\n",
    "            else:\n",
    "                Load_Name = file_name.split('_')[1] + '_' + file_name.split('_')[3] + '_' + file_name.split('_')[5] + '_' + file_name.split('_')[6][:-4] # To remove the .csv string at the end\n",
    "\n",
    "        print(f'Aggregating tag data from {file_name} with load name {Load_Name}')\n",
    "\n",
    "        return pd.read_csv(\n",
    "            os.path.join(folder_path, file_name),\n",
    "            skiprows=4\n",
    "        ).rename(\n",
    "            columns={\n",
    "                'Unnamed: 0': 'Date'\n",
    "            }\n",
    "        ).rename(\n",
    "            columns=lambda x: x.strip() # Adding this since some files have spaces in the column names\n",
    "        ).assign(\n",
    "            Load_Name=Load_Name\n",
    "        )\n",
    "    \n",
    "    elif file_type == 'excel':\n",
    "        \n",
    "        if 'Eligible' not in file_name:\n",
    "            Load_Name = file_name.split('_')[0] + '_' + file_name.split('_')[1].split('-')[0]\n",
    "        else:\n",
    "            Load_Name = file_name.split('_')[0] + '_' + file_name.split('_')[1].split('-')[1]        \n",
    "\n",
    "        print(f'Aggregating tag data from {file_name} with load name {Load_Name}')\n",
    "\n",
    "        return pd.read_excel(\n",
    "            os.path.join(folder_path, file_name),\n",
    "            engine='pyxlsb',\n",
    "            skiprows=5\n",
    "        ).rename(\n",
    "            columns={\n",
    "                'Unnamed: 0': 'Date'\n",
    "            }\n",
    "        ).rename(\n",
    "            columns=lambda x: x.strip() # Adding this since some files have spaces in the column names\n",
    "        ).assign(\n",
    "            Load_Name=Load_Name,\n",
    "            Date=lambda DF: pd.to_datetime('1899-12-30') + pd.to_timedelta(DF.Date, unit='D') if 'xlsb' in file_name else pd.to_datetime(DF.Date) # Adding this since pandas messes up reading dates from xlsb files sometimes\n",
    "        )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af141c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags = pd.DataFrame()\n",
    "\n",
    "for folder_name in folder_names:\n",
    "    \n",
    "    folder_path = os.path.join(monthly_data_folder_path, folder_name, 'CIEP') # Only aggregating CIEP data\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "\n",
    "        if 'PLA' in file_name: # Adding this filter only to aggregate tags\n",
    "            \n",
    "            if 'Recent' in folder_name:\n",
    "                df_tags_temp = aggregate_tags(folder_path, file_name, 'excel') # Newer files in excel binary\n",
    "            else:\n",
    "                df_tags_temp = aggregate_tags(folder_path, file_name, 'csv') # Older files in csv\n",
    "        \n",
    "            df_tags = pd.concat([df_tags, df_tags_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f864bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags_pivoted = df_tags.assign(\n",
    "    Date=lambda DF: pd.to_datetime(DF.Date, errors='coerce') # There are some missing dates - hence coercing the errors and then interpolating\n",
    ").interpolate().rename(\n",
    "    columns={\n",
    "        'Capacity Peak Load Allocation': 'PLC',\n",
    "        'Transmission Peak Load Allocation': 'NSPL'\n",
    "    }\n",
    ").drop_duplicates().pivot_table( # Drops just 1 row in RECO Eligible\n",
    "    index='Date',\n",
    "    columns='Load_Name',\n",
    "    values=['PLC', 'NSPL']\n",
    ").pipe(\n",
    "    lambda DF: DF[DF.index >= '2014-01-01'] # Filtering out data before 2014. Raw data starts from 2001-08-01\n",
    ")\n",
    "\n",
    "df_tags_pivoted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e08ef2",
   "metadata": {},
   "source": [
    "Fixing outliers in tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f754583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tags_pivoted[('PLC', 'RECO_BGS')].loc[\n",
    "#     lambda DF: DF.index == '2023-11-09'\n",
    "# ] = 0.5 * df_tags_pivoted[('PLC', 'RECO_BGS')].loc[\n",
    "#     lambda DF: DF.index == '2023-11-08'\n",
    "# ] + 0.5 * df_tags_pivoted[('PLC', 'RECO_BGS')].loc[\n",
    "#     lambda DF: DF.index == '2023-11-10'\n",
    "# ]\n",
    "\n",
    "df_tags_pivoted.loc['2023-11-09', ('PLC', 'RECO_BGS')] = (\n",
    "    0.5 * df_tags_pivoted.loc['2023-11-08', ('PLC', 'RECO_BGS')] +\n",
    "    0.5 * df_tags_pivoted.loc['2023-11-10', ('PLC', 'RECO_BGS')]\n",
    ")\n",
    "\n",
    "df_tags_pivoted.loc['2023-11-09', ('PLC', 'RECO_Eligible')] = (\n",
    "    0.5 * df_tags_pivoted.loc['2023-11-08', ('PLC', 'RECO_Eligible')] +\n",
    "    0.5 * df_tags_pivoted.loc['2023-11-10', ('PLC', 'RECO_Eligible')]\n",
    ")\n",
    "\n",
    "df_tags_pivoted.loc['2017-07-06', ('PLC', 'ACE_Eligible')] = (\n",
    "    0.5 * df_tags_pivoted.loc['2017-07-05', ('PLC', 'ACE_Eligible')] +\n",
    "    0.5 * df_tags_pivoted.loc['2017-07-07', ('PLC', 'ACE_Eligible')]\n",
    ")\n",
    "\n",
    "df_tags_pivoted.loc['2017-07-06', ('NSPL', 'ACE_Eligible')] = (\n",
    "    0.5 * df_tags_pivoted.loc['2017-07-05', ('NSPL', 'ACE_Eligible')] +\n",
    "    0.5 * df_tags_pivoted.loc['2017-07-07', ('NSPL', 'ACE_Eligible')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c894d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags_pivoted = df_tags_pivoted.pipe(\n",
    "    flatten_columns\n",
    ").reindex(\n",
    "    columns=['PLC_' + zone + '_BGS' for zone in zones_without_gen] \n",
    "    + ['NSPL_' + zone + '_BGS' for zone in zones_without_gen] \n",
    "    + ['PLC_' + zone + '_Eligible' for zone in zones_without_gen] \n",
    "    + ['NSPL_' + zone + '_Eligible' for zone in zones_without_gen]\n",
    ")\n",
    "\n",
    "df_tags_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda6f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for zone in zones_without_gen:\n",
    "    fig = px.line(\n",
    "        df_tags_pivoted.reset_index(),\n",
    "        x='Date',\n",
    "        y=[f'PLC_{zone}_Eligible', f'NSPL_{zone}_Eligible'],\n",
    "        title=f'PLC/NSPL_{zone}_Eligible tags'\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1486d3",
   "metadata": {},
   "source": [
    "PLC:\n",
    "\n",
    "a. ACE, PSEG, RECO - Unscaled\n",
    "\n",
    "b. JCP&L - Need to look into\n",
    "\n",
    "NSPL\n",
    "\n",
    "a. ACE, PSEG, RECO - Scaled (aka scaling factors = 1)\n",
    "\n",
    "b. JCP&L - Need to look into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47fb108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "987606ea",
   "metadata": {},
   "source": [
    "### Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa37856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_counts(file_path):\n",
    "    \n",
    "    if 'xlsb' in file_path:\n",
    "        engine='pyxlsb' # More recent files in xlsb\n",
    "    else:\n",
    "        engine=None\n",
    "\n",
    "    counts_data =  pd.read_excel(\n",
    "        file_path,\n",
    "        engine=engine,\n",
    "        skiprows=4,\n",
    "        usecols='B:I',\n",
    "        sheet_name=None,\n",
    "        nrows=5\n",
    "    )\n",
    "\n",
    "    # Adding this since the sheet and column names have spaces sometimes\n",
    "    counts_data = {\n",
    "        sheet.strip(): DF.rename(columns=str.strip) for sheet, DF in counts_data.items()\n",
    "    }\n",
    "\n",
    "    df_counts = pd.concat(counts_data, names=['Month', 'Row']).reset_index()\n",
    "\n",
    "    return df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b878b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_switching_file_name = 'Hist_CIEP_Switching_Statistics.xlsx'\n",
    "\n",
    "counts_historical_file_path = os.path.join(monthly_data_folder_path, switching_folder_name, historical_switching_file_name)\n",
    "\n",
    "df_counts_historical = aggregate_counts(counts_historical_file_path)\n",
    "\n",
    "# df_counts_historical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadc111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_switching_file_name = 'CIEP_Switching_Historical_December_2025_Update.xlsb'\n",
    "\n",
    "recent_counts_file_path = os.path.join(monthly_data_folder_path, recent_folder_name, recent_switching_file_name)\n",
    "\n",
    "df_counts_recent = aggregate_counts(recent_counts_file_path)\n",
    "\n",
    "# df_counts_recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42a9a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = pd.concat([df_counts_historical, df_counts_recent])\n",
    "\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19331494",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_pivoted = df_counts.drop(\n",
    "    columns=['Row', 'Unnamed: 5']\n",
    ").rename(\n",
    "    columns={\n",
    "        'Unnamed: 1': 'Zone',\n",
    "        'Total': 'Eligible_counts',\n",
    "        'Switching': 'Switching_counts',\n",
    "        'Percentage': 'Percentage_counts',\n",
    "        'Total.1': 'Eligible_load',\n",
    "        'Switching.1': 'Switching_load',\n",
    "        'Percentage.1': 'Percentage_load'\n",
    "    }\n",
    ").assign(\n",
    "    Month=lambda DF: pd.to_datetime(DF.Month, format='%B %Y'),\n",
    "    Zone=lambda DF: DF.Zone.replace( \n",
    "        {\n",
    "            'PSE&G*': 'PSE&G', # These difference are only in the very old data\n",
    "            'Conectiv': 'ACE'\n",
    "        }\n",
    "    )\n",
    ").pivot_table(\n",
    "    index='Month',\n",
    "    columns='Zone',\n",
    "    values=['Eligible_counts', 'Switching_counts', 'Percentage_counts', 'Eligible_load', 'Switching_load', 'Percentage_load']\n",
    ").pipe(\n",
    "    lambda DF: DF[DF.index >= '2013-12-01'] # Filering out data before 2014. Raw data starts in August 2003\n",
    ").interpolate()\n",
    "\n",
    "df_counts_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa79445",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_pivoted.xs(\n",
    "    'Eligible_counts', axis=1, level=0\n",
    ").pipe(\n",
    "    px.line\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3d0379",
   "metadata": {},
   "source": [
    "### Deration Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef4d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deration_factors = pd.DataFrame()\n",
    "\n",
    "for folder_name in folder_names:\n",
    "    folder_path = os.path.join(monthly_data_folder_path, folder_name) \n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if ('Derating' in file_name) or ('De-Rating' in file_name):\n",
    "            \n",
    "            print(f'Aggregating deration factors from {file_name}')\n",
    "            \n",
    "            if 'csv' in file_name:\n",
    "                df_deration_factors_temp = pd.read_csv(\n",
    "                    os.path.join(folder_path, file_name),\n",
    "                    skiprows=4\n",
    "                ).assign(\n",
    "                    Load_Name = file_name.split('_')[1],\n",
    "                    Date=lambda DF: pd.to_datetime(DF.Date)\n",
    "                ).rename(\n",
    "                    columns={\n",
    "                        'Loss De-Rating Factor ': 'Deration_Factor',\n",
    "                        'HE': 'Hour'\n",
    "                    }\n",
    "                )\n",
    "            \n",
    "            elif 'xlsb' in file_name:\n",
    "                \n",
    "                df_deration_factors_temp = pd.read_excel(\n",
    "                    os.path.join(folder_path, file_name),\n",
    "                    engine='pyxlsb',\n",
    "                    skiprows=5\n",
    "                ).assign(\n",
    "                    Load_Name = file_name.split('_')[0],\n",
    "                    # Date=lambda DF: pd.to_datetime(DF.Date)\n",
    "                    Date=lambda DF: pd.to_datetime('1899-12-30') + pd.to_timedelta(DF.Date, unit='D'), # Adding this to handle issues reading xlsb files\n",
    "                ).rename(\n",
    "                    columns={\n",
    "                        'Loss De-Rating Factor ': 'Deration_Factor',\n",
    "                        'HE': 'Hour'\n",
    "                    }\n",
    "                )                \n",
    "\n",
    "            df_deration_factors = pd.concat([df_deration_factors, df_deration_factors_temp])\n",
    "\n",
    "df_deration_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d733328",
   "metadata": {},
   "source": [
    "Doing the DST adjustments for these 3 zones. JCPL has some other issues so is treated separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b12de",
   "metadata": {},
   "outputs": [],
   "source": [
    "deration_factors_dict = {}\n",
    "\n",
    "for zone in ['ACE', 'PSEG', 'RECO']:\n",
    "    deration_factors_dict[zone] = df_deration_factors.loc[\n",
    "        lambda DF: DF.Load_Name == zone\n",
    "    ].loc[\n",
    "        lambda DF: DF.Date >= '2014-01-01'\n",
    "    ].drop(\n",
    "        columns='Load_Name'\n",
    "    ).set_index(\n",
    "        ['Date', 'Hour']\n",
    "    ).pipe(\n",
    "        handle_dst_dates,\n",
    "        dst_end_method='drop',\n",
    "        he2_to_drop='first',\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd20724",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deration_factors_pivoted = deration_factors_dict['ACE'].merge(\n",
    "    deration_factors_dict['PSEG'],\n",
    "    left_index=True,\n",
    "    right_index=True\n",
    ").merge(\n",
    "    deration_factors_dict['RECO'],\n",
    "    left_index=True,\n",
    "    right_index=True    \n",
    ").rename(\n",
    "    columns={\n",
    "        'Deration_Factor_x': 'ACE',\n",
    "        'Deration_Factor_y': 'PSEG',\n",
    "        'Deration_Factor': 'RECO'\n",
    "    }\n",
    ")\n",
    "\n",
    "df_deration_factors_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c245e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deration_factors_pivoted = df_deration_factors_pivoted.merge(df_deration_factors.loc[\n",
    "        lambda DF: DF.Load_Name == 'JCP&L'\n",
    "    ].loc[\n",
    "        lambda DF: DF.Date >= '2014-01-01'\n",
    "    ].drop(\n",
    "        columns='Load_Name'\n",
    "    ).assign(\n",
    "        Date=lambda DF: pd.to_datetime(DF.Date.dt.date)\n",
    "    ).set_index(\n",
    "        ['Date', 'Hour']\n",
    "    ).pipe(\n",
    "        handle_dst_dates,\n",
    "        dst_end_method='use_he_25',\n",
    "        # he2_to_drop='first',\n",
    "        verbose=True\n",
    "    ),\n",
    "    left_index=True,\n",
    "    right_index=True\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'Deration_Factor': 'JCP&L'\n",
    "        }\n",
    "    ).reindex(\n",
    "        columns=['ACE', 'JCP&L', 'PSEG', 'RECO']\n",
    "    )\n",
    "\n",
    "df_deration_factors_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917a19b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Followed this approach originally\n",
    "\n",
    "# df_deration_factors_pivoted = df_deration_factors.pivot_table( # Using pivot table to groupby since there are some duplicate entries \n",
    "#     index=['Date', 'Hour'],\n",
    "#     columns='Load_Name',\n",
    "#     values='Deration_Factor'\n",
    "# ).pipe(\n",
    "#     lambda DF: DF[DF.index.get_level_values(0) >= '2014-01-01'] # filtering out historical data before 2014-01-01\n",
    "# ).reindex(\n",
    "#     hourly_index('2014-01-01', '2025-11-30') # Doing this since there are some HE 25 and missing data. May have to update the end date if more data is posted\n",
    "# ).interpolate()\n",
    "\n",
    "# df_deration_factors_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb6f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deration_factors_pivoted.reset_index().pipe(\n",
    "    px.line,\n",
    "    x='Date',\n",
    "    y=zones_without_gen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5003bc55",
   "metadata": {},
   "source": [
    "### Scaling Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7913645",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaling_factors = pd.DataFrame()\n",
    "\n",
    "for folder_name in folder_names:\n",
    "    \n",
    "    folder_path = os.path.join(monthly_data_folder_path, folder_name) \n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        \n",
    "        if 'Scaling' in file_name:\n",
    "\n",
    "            print(f'Aggregating scaling factors from {file_name}')\n",
    "\n",
    "            zone = file_name.split('_')[0]\n",
    "\n",
    "            if zone in ['ACE', 'PSEG', 'RECO']: # For these zones, the NSPL scaling factor is 1\n",
    "                df_scaling_factors_temp = pd.read_excel(\n",
    "                    os.path.join(folder_path, file_name),\n",
    "                    engine='pyxlsb',\n",
    "                    skiprows=5\n",
    "                ).assign(\n",
    "                    Load_Name=zone,\n",
    "                    Type='PLC'   \n",
    "                )\n",
    "\n",
    "                df_scaling_factors = pd.concat([df_scaling_factors, df_scaling_factors_temp])\n",
    "\n",
    "            else: # JCPL\n",
    "                df_scaling_factors_temp_1 = pd.read_excel(\n",
    "                    os.path.join(folder_path, file_name),\n",
    "                    engine='pyxlsb',\n",
    "                    skiprows=5,\n",
    "                    sheet_name='CAP DZSF'\n",
    "                ).assign(\n",
    "                    Load_Name=zone,\n",
    "                    Type='PLC'\n",
    "                )\n",
    "\n",
    "                df_scaling_factors_temp_2 = pd.read_excel(\n",
    "                    os.path.join(folder_path, file_name),\n",
    "                    engine='pyxlsb',\n",
    "                    skiprows=5,\n",
    "                    sheet_name='NSPL DZSF'\n",
    "                ).assign(\n",
    "                    Load_Name=zone,\n",
    "                    Type='NSPL'\n",
    "                )\n",
    "\n",
    "                df_scaling_factors = pd.concat([df_scaling_factors, df_scaling_factors_temp_1, df_scaling_factors_temp_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04432ec9",
   "metadata": {},
   "source": [
    "Pulling RECO NSPL SF from internal file since the data is not published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6f4378",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_dzfs_path = r'K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\Capacity\\Zonal Scaling Factors\\PJM Daily Zonal Scaling Factors v3.xlsb'\n",
    "\n",
    "df_nspl_reco_scaling_factors = pd.read_excel(\n",
    "    internal_dzfs_path,\n",
    "    sheet_name='NSPL DZSF',\n",
    "    skiprows=1,\n",
    "    usecols='B:O'\n",
    ").assign(\n",
    "    Date=lambda DF: pd.to_datetime(DF.Date, origin='1899-12-30', unit='D')\n",
    ")[['Date', 'RECO']].set_index(\n",
    "    'Date'\n",
    ").set_axis(\n",
    "    pd.MultiIndex.from_tuples(\n",
    "    [('NSPL', 'RECO')]\n",
    "), \n",
    "axis='columns'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da6a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaling_factors_pivoted = df_scaling_factors.drop(\n",
    "    columns=['Unnamed: 2', 'Unnamed: 3']\n",
    ").assign(\n",
    "    Date=lambda DF: pd.to_datetime(DF.Date, unit='D', origin='1899-12-30') # Converting excel date to datetime object\n",
    ").pivot(\n",
    "    index='Date',\n",
    "    columns=['Type', 'Load_Name'],\n",
    "    values='DZSF'\n",
    ").pipe(\n",
    "    lambda DF: DF[DF.index >= '2014-01-01'] # Filtering out data before 2014. Raw data starts from 2011-06-01\n",
    ").join(\n",
    "    df_nspl_reco_scaling_factors\n",
    ").replace(\n",
    "    np.nan, 1 # filling the missing NSPL scaling factors for JCP&L and RECO\n",
    ")\n",
    "\n",
    "df_scaling_factors_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c70ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaling_factors_pivoted.pipe(\n",
    "    flatten_columns\n",
    ").pipe(\n",
    "    px.line\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0873729",
   "metadata": {},
   "source": [
    "### Checking internal tags to see if they are scaled or unscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828443d5",
   "metadata": {},
   "source": [
    "Internal data pulled using \"K:\\Valuation\\_Analysts\\Hemanth\\Python Notebooks\\Deals\\Download-MSRS-Reports.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73de5f3",
   "metadata": {},
   "source": [
    "Note: \n",
    "\n",
    "BGS-Total = BGS-CIEP + BGS-RSCP\n",
    "\n",
    "Eligible-Total = Eligible-CIEP + Eligible-RSCP. I think Retail-Total is the same as Eligible-Total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db14a59a",
   "metadata": {},
   "source": [
    "#### JCP&L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600087e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "msrs_folder_path = r\"K:\\Valuation\\Wholesale for Retail\\_Full Requirements\\PJM\\BGS FP & CIEP\\2026-02\\CIEP\\Pri Hemanth\\Data\\msrs_reports_recon_jcpl\"\n",
    "\n",
    "df_internal_tags = pd.DataFrame()\n",
    "\n",
    "for file_name in os.listdir(msrs_folder_path):\n",
    "    if ('Locational Reliability Charge Summary' in file_name) or ('NITS Charge Summary' in file_name):\n",
    "        \n",
    "        print(f'Aggregating internal tags from {file_name}')\n",
    "        \n",
    "        df_internal_tags_temp = pd.read_csv(\n",
    "            os.path.join(msrs_folder_path, file_name),\n",
    "            skiprows=4\n",
    "        )\n",
    "\n",
    "        df_internal_tags = pd.concat([df_internal_tags, df_internal_tags_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ec43ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling appropriate RR from capacity forecast file\n",
    "\n",
    "rr_jcpl_py_20_21, rr_jcpl_py_21_22 = pd.read_excel(\n",
    "    r'K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\Capacity\\Capacity FORECAST ALL - CURRENT.xlsx',\n",
    "    sheet_name='Sheet1',\n",
    "    skiprows=24,\n",
    "    usecols='A:U'\n",
    ").dropna(how='all').iloc[\n",
    "    10, [14, 18]\n",
    "].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a869cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_internal_tags[\n",
    "    ['Date', 'UCAP Obligation (MW)', 'Daily Peak Load (MW)']\n",
    "].dropna(how='all').assign(\n",
    "    Date=lambda DF: pd.to_datetime(DF.Date)\n",
    ").sort_values(\n",
    "    by='Date'\n",
    ").assign(\n",
    "    Planning_year=lambda DF: DF.Date.apply(planning_year),\n",
    "    RR=lambda DF: DF.apply(lambda DF: rr_jcpl_py_20_21 if DF.Planning_year == '2020-2021' else rr_jcpl_py_21_22, axis=1),\n",
    "    percent_served = lambda DF: DF.apply(lambda DF: 1 / 6 if DF.Planning_year == '2020-2021' else 0.3, axis=1), # These percentages served are exact and have been verified from prior workups\n",
    "    Internal_PLC_Scaled=lambda DF: DF['UCAP Obligation (MW)'] / (DF.RR * DF.percent_served),\n",
    "    Internal_NSPL_Scaled=lambda DF: DF['Daily Peak Load (MW)'] / DF.percent_served\n",
    ").merge(\n",
    "    df_tags_pivoted[['PLC_JCP&L_BGS', 'NSPL_JCP&L_BGS']].reset_index(),\n",
    "on='Date'\n",
    ").pipe(\n",
    "    px.scatter,\n",
    "    x='Date',\n",
    "    # y=['Internal_PLC_Scaled', 'PLC_JCP&L_BGS'],\n",
    "    y=['Internal_NSPL_Scaled', 'NSPL_JCP&L_BGS'],\n",
    "    title='Internal vs posted NSPL comparison'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c49669",
   "metadata": {},
   "source": [
    "#### PSEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8bbaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "msrs_folder_path = r\"K:\\Valuation\\Wholesale for Retail\\_Full Requirements\\PJM\\BGS FP & CIEP\\2026-02\\CIEP\\Pri Hemanth\\Data\\msrs_reports_recon_pseg\"\n",
    "\n",
    "df_internal_tags = pd.DataFrame()\n",
    "\n",
    "for file_name in os.listdir(msrs_folder_path):\n",
    "    if ('Locational Reliability Charge Summary' in file_name) or ('NITS Charge Summary' in file_name):\n",
    "        \n",
    "        print(f'Aggregating internal tags from {file_name}')\n",
    "        \n",
    "        df_internal_tags_temp = pd.read_csv(\n",
    "            os.path.join(msrs_folder_path, file_name),\n",
    "            skiprows=4\n",
    "        )\n",
    "\n",
    "        df_internal_tags = pd.concat([df_internal_tags, df_internal_tags_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c9bc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling appropriate RR from capacity forecast file\n",
    "\n",
    "rr_pseg_py_20_21, rr_pseg_py_21_22, rr_pseg_py_22_23, rr_pseg_py_23_24, rr_pseg_py_24_25 = pd.read_excel(\n",
    "    r'K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\Capacity\\Capacity FORECAST ALL - CURRENT.xlsx',\n",
    "    sheet_name='Sheet1',\n",
    "    skiprows=24,\n",
    "    usecols='A:U'\n",
    ").dropna(how='all').iloc[\n",
    "    15, 14:19\n",
    "].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac25897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def pseg_rr(DF):\n",
    "    if DF.Planning_year == '2020-2021':\n",
    "        return rr_pseg_py_20_21\n",
    "    elif DF.Planning_year == '2021-2022':\n",
    "        return rr_pseg_py_21_22\n",
    "    elif DF.Planning_year == '2022-2023':\n",
    "        return rr_pseg_py_22_23\n",
    "    elif DF.Planning_year == '2023-2024':\n",
    "        return rr_pseg_py_23_24\n",
    "    elif DF.Planning_year == '2024-2025':\n",
    "        return rr_pseg_py_24_25\n",
    "    \n",
    "def pseg_percent_served(DF):\n",
    "    if DF.Planning_year == '2020-2021':\n",
    "        return 6 / 24\n",
    "    elif DF.Planning_year == '2021-2022':\n",
    "        return 10 / 24\n",
    "    elif DF.Planning_year == '2022-2023':\n",
    "        return 10 / 22\n",
    "    elif DF.Planning_year == '2023-2024':\n",
    "        return 2 / 22\n",
    "    elif DF.Planning_year == '2024-2025':\n",
    "        return 3 / 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f083b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_internal_tags[\n",
    "    ['Date', 'UCAP Obligation (MW)', 'Daily Peak Load (MW)']\n",
    "].dropna(how='all').assign(\n",
    "    Date=lambda DF: pd.to_datetime(DF.Date)\n",
    ").sort_values(\n",
    "    by='Date'\n",
    ").assign(\n",
    "    Planning_year=lambda DF: DF.Date.apply(planning_year),\n",
    "    RR=lambda DF: DF.apply(pseg_rr, axis=1),\n",
    "    percent_served = lambda DF: DF.apply(pseg_percent_served, axis=1), # These percentages served are exact and have been verified from prior workups\n",
    "    Internal_PLC_Scaled=lambda DF: DF['UCAP Obligation (MW)'] / (DF.RR * DF.percent_served),\n",
    "    Internal_NSPL_Scaled=lambda DF: DF['Daily Peak Load (MW)'] / DF.percent_served\n",
    ").merge(\n",
    "    df_tags_pivoted[['PLC_PSEG_BGS', 'NSPL_PSEG_BGS']].reset_index(),\n",
    "on='Date'\n",
    ").pipe(\n",
    "    px.scatter,\n",
    "    x='Date',\n",
    "    y=['Internal_PLC_Scaled', 'PLC_PSEG_BGS'],\n",
    "    # y=['Internal_NSPL_Scaled', 'NSPL_PSEG_BGS'],\n",
    "    title='Internal vs posted PLC comparison'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f53395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c36145b",
   "metadata": {},
   "source": [
    "#### RECO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f177680",
   "metadata": {},
   "outputs": [],
   "source": [
    "msrs_folder_path = r\"K:\\Valuation\\Wholesale for Retail\\_Full Requirements\\PJM\\BGS FP & CIEP\\2026-02\\CIEP\\Pri Hemanth\\Data\\msrs_reports_recon_reco\"\n",
    "\n",
    "df_internal_tags = pd.DataFrame()\n",
    "\n",
    "for file_name in os.listdir(msrs_folder_path):\n",
    "    if ('Locational Reliability Charge Summary' in file_name) or ('NITS Charge Summary' in file_name):\n",
    "        \n",
    "        print(f'Aggregating internal tags from {file_name}')\n",
    "        \n",
    "        df_internal_tags_temp = pd.read_csv(\n",
    "            os.path.join(msrs_folder_path, file_name),\n",
    "            skiprows=4\n",
    "        )\n",
    "\n",
    "        df_internal_tags = pd.concat([df_internal_tags, df_internal_tags_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e15c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_reco_22_23 = pd.read_excel(\n",
    "    r'K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\Capacity\\Capacity FORECAST ALL - CURRENT.xlsx',\n",
    "    sheet_name='Sheet1',\n",
    "    skiprows=24,\n",
    "    usecols='A:U'\n",
    ").dropna(how='all').iloc[\n",
    "    16, 16\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fb4c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_internal_tags[\n",
    "    ['Date', 'UCAP Obligation (MW)', 'Daily Peak Load (MW)']\n",
    "].dropna(how='all').assign(\n",
    "    Date=lambda DF: pd.to_datetime(DF.Date)\n",
    ").sort_values(\n",
    "    by='Date'\n",
    ").assign(\n",
    "    Planning_year=lambda DF: DF.Date.apply(planning_year),\n",
    "    RR=lambda DF: rr_reco_22_23,\n",
    "    percent_served = 1, # These percentages served are exact and have been verified from prior workups\n",
    "    Internal_PLC_Scaled=lambda DF: DF['UCAP Obligation (MW)'] / (DF.RR * DF.percent_served),\n",
    "    Internal_NSPL_Scaled=lambda DF: DF['Daily Peak Load (MW)'] / DF.percent_served\n",
    ").merge(\n",
    "    df_tags_pivoted[['PLC_RECO_BGS', 'NSPL_RECO_BGS']].reset_index(),\n",
    "on='Date'\n",
    ").pipe(\n",
    "    px.scatter,\n",
    "    x='Date',\n",
    "    # y=['Internal_PLC_Scaled', 'PLC_RECO_BGS'],\n",
    "    y=['Internal_NSPL_Scaled', 'NSPL_RECO_BGS'],\n",
    "    title='Internal vs posted NSPL comparison'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ac65e3",
   "metadata": {},
   "source": [
    "### Doing some checks on the tags to see if they sum up correctly to the totals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eeebb2",
   "metadata": {},
   "source": [
    "Conclusion of all this analysis: Hi CIEP team. I have done the following checks on the tags (in addition to the checks on JCPL tags we did above).\n",
    " \n",
    "The BGS CIEP and BGS RSCP tags sum up to the BGS TOTAL tags (makes sense)\n",
    "The BGS CIEP Eligible tags and the BGS RSCP Eligible tags sum up to the Retail TOTAL tags without scaling. (This means that their claim that the BGS CIEP Eligible tags need to be scaled is incorrect - this seems to have been documented by Kevin O as well last time).\n",
    "So I propose we treat the tags in the following way:\n",
    " \n",
    "ACE, RECO, PSEG - no ambiguity - PLC unscaled and NSPL scaled (both BGS and Eligible)\n",
    " \n",
    "JCPL - \n",
    "BGS PLC unscaled (matches what they say), \n",
    "Eligible PLC already scaled (contrary to what they say). \n",
    "BGS NSPL unscaled (matches what they say), \n",
    "Eligible NSPL already scaled (contrary to what they say)\n",
    " \n",
    "Is this fair?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b170aa",
   "metadata": {},
   "source": [
    "Aggregating RSCP and TOTAL tags and pivoting to the required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f1789",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rscp_tags = pd.DataFrame()\n",
    "df_total_tags = pd.DataFrame()\n",
    "\n",
    "for folder_name in folder_names_without_recent:\n",
    "    \n",
    "    folder_path = os.path.join(monthly_data_folder_path, folder_name, 'RSCP') # Aggregating RSCP data\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "\n",
    "        if 'PLA' in file_name: # Adding this filter only to aggregate tags\n",
    "            \n",
    "            if 'Recent' in folder_name:\n",
    "                df_rscp_tags_temp = aggregate_tags(folder_path, file_name, 'excel') # Newer files in excel binary\n",
    "            else:\n",
    "                df_rscp_tags_temp = aggregate_tags(folder_path, file_name, 'csv') # Older files in csv\n",
    "        \n",
    "            df_rscp_tags = pd.concat([df_rscp_tags, df_rscp_tags_temp])\n",
    "\n",
    "    folder_path = os.path.join(monthly_data_folder_path, folder_name, 'TOTAL') # Aggregating RSCP data\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "\n",
    "        if 'PLA' in file_name: # Adding this filter only to aggregate tags\n",
    "            \n",
    "            if 'Recent' in folder_name:\n",
    "                df_total_tags_temp = aggregate_tags(folder_path, file_name, 'excel') # Newer files in excel binary\n",
    "            else:\n",
    "                df_total_tags_temp = aggregate_tags(folder_path, file_name, 'csv') # Older files in csv\n",
    "        \n",
    "            df_total_tags = pd.concat([df_total_tags, df_total_tags_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6baed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rscp_tags_pivoted = df_rscp_tags.assign(\n",
    "    Date=lambda DF: pd.to_datetime(DF.Date, errors='coerce') # There are some missing dates - hence coercing the errors and then interpolating\n",
    ").interpolate().rename(\n",
    "    columns={\n",
    "        'Capacity Peak Load Allocation': 'PLC',\n",
    "        'Transmission Peak Load Allocation': 'NSPL'\n",
    "    }\n",
    ").drop_duplicates().pivot_table( # Drops just 1 row in RECO Eligible\n",
    "    index='Date',\n",
    "    columns='Load_Name',\n",
    "    values=['PLC', 'NSPL']\n",
    ").pipe(\n",
    "    lambda DF: DF[DF.index >= '2014-01-01'] # Filtering out data before 2014. Raw data starts from 2001-08-01\n",
    ")\n",
    "\n",
    "df_rscp_tags_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd674b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_tags_pivoted = df_total_tags.assign(\n",
    "    Date=lambda DF: pd.to_datetime(DF.Date, errors='coerce') # There are some missing dates - hence coercing the errors and then interpolating\n",
    ").interpolate().rename(\n",
    "    columns={\n",
    "        'Capacity Peak Load Allocation': 'PLC',\n",
    "        'Transmission Peak Load Allocation': 'NSPL'\n",
    "    }\n",
    ").drop_duplicates().pivot_table( # Drops just 1 row in RECO Eligible\n",
    "    index='Date',\n",
    "    columns='Load_Name',\n",
    "    values=['PLC', 'NSPL']\n",
    ").pipe(\n",
    "    lambda DF: DF[DF.index >= '2014-01-01'] # Filtering out data before 2014. Raw data starts from 1999-11-01\n",
    ")\n",
    "\n",
    "df_total_tags_pivoted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e170735",
   "metadata": {},
   "source": [
    "Adding CIEP and RSCP Eligible tags and comparing to the total eligible/retail. Conclusion - tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb10678",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'This is a check on the NSPL tags: \\n')\n",
    "\n",
    "for zone in zones_without_gen:\n",
    "\n",
    "    print(f'Checking if the sum of CIEP and RSCP Eligible NSPL tags sum up to the TOTAL RETAIL and printing the min, max and mean of the delta for zone {zone} \\n')\n",
    "\n",
    "    print(df_tags_pivoted.loc[\n",
    "        lambda DF: DF.index <= '2022-05-31'\n",
    "    ][['NSPL_' + zone + '_Eligible']].rename(\n",
    "        columns={\n",
    "            'NSPL_' + zone + '_Eligible': 'NSPL_' + zone + '_Eligible_CIEP'\n",
    "        }\n",
    "    ).merge(\n",
    "        right=df_rscp_tags_pivoted.pipe(\n",
    "        flatten_columns\n",
    "    )[['NSPL_' + zone + '_Eligible']].rename(\n",
    "        columns={\n",
    "            'NSPL_' + zone + '_Eligible': 'NSPL_' + zone + '_Eligible_RSCP'\n",
    "        }\n",
    "    ),\n",
    "    on='Date'\n",
    "    ).merge(\n",
    "        right=df_total_tags_pivoted.pipe(\n",
    "        flatten_columns\n",
    "    )[['NSPL_' + zone + '_Retail']],\n",
    "    on='Date'\n",
    "    ).assign(\n",
    "        Check=lambda DF: DF['NSPL_' + zone + '_Eligible_CIEP'] + DF['NSPL_' + zone + '_Eligible_RSCP'] - DF['NSPL_' + zone + '_Retail']\n",
    "    ).Check.describe().loc[['min', 'max', 'mean']])\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e43ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'This is a way to check on the PLC tags: \\n')\n",
    "\n",
    "for zone in zones_without_gen:\n",
    "\n",
    "    print(f'Checking if the sum of CIEP and RSCP Eligible PLC tags sum up to the TOTAL RETAIL and printing the min, max and mean of the delta for zone {zone} \\n')\n",
    "\n",
    "    print(df_tags_pivoted.loc[\n",
    "        lambda DF: DF.index <= '2022-05-31'\n",
    "    ][['PLC_' + zone + '_Eligible']].rename(\n",
    "        columns={\n",
    "            'PLC_' + zone + '_Eligible': 'PLC_' + zone + '_Eligible_CIEP'\n",
    "        }\n",
    "    ).merge(\n",
    "        right=df_rscp_tags_pivoted.pipe(\n",
    "        flatten_columns\n",
    "    )[['PLC_' + zone + '_Eligible']].rename(\n",
    "        columns={\n",
    "            'PLC_' + zone + '_Eligible': 'PLC_' + zone + '_Eligible_RSCP'\n",
    "        }\n",
    "    ),\n",
    "    on='Date'\n",
    "    ).merge(\n",
    "        right=df_total_tags_pivoted.pipe(\n",
    "        flatten_columns\n",
    "    )[['PLC_' + zone + '_Retail']],\n",
    "    on='Date'\n",
    "    ).assign(\n",
    "        Check=lambda DF: DF['PLC_' + zone + '_Eligible_CIEP'] + DF['PLC_' + zone + '_Eligible_RSCP'] - DF['PLC_' + zone + '_Retail']\n",
    "    ).Check.describe().loc[['min', 'max', 'mean']])\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa4b2a",
   "metadata": {},
   "source": [
    "Adding CIEP and RSCP BGS tags and comparing to the total BGS - Conclusion - tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5a8bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for zone in zones_without_gen:\n",
    "\n",
    "    print(f'Checking if the sum of CIEP and RSCP BGS NSPL tags sum up to the TOTAL BGS and printing the min, max and mean of the delta for zone {zone} \\n')\n",
    "\n",
    "    print(df_tags_pivoted.loc[\n",
    "        lambda DF: DF.index <= '2022-05-31'\n",
    "    ][['NSPL_' + zone + '_BGS']].rename(\n",
    "        columns={\n",
    "            'NSPL_' + zone + '_BGS': 'NSPL_' + zone + '_BGS_CIEP'\n",
    "        }\n",
    "    ).merge(\n",
    "        right=df_rscp_tags_pivoted.pipe(\n",
    "        flatten_columns\n",
    "    )[['NSPL_' + zone + '_BGS']].rename(\n",
    "        columns={\n",
    "            'NSPL_' + zone + '_BGS': 'NSPL_' + zone + '_BGS_RSCP'\n",
    "        }\n",
    "    ),\n",
    "    on='Date'\n",
    "    ).merge(\n",
    "        right=df_total_tags_pivoted.pipe(\n",
    "        flatten_columns\n",
    "    )[['NSPL_' + zone + '_Total']],\n",
    "    on='Date'\n",
    "    ).assign(\n",
    "        Check=lambda DF: DF['NSPL_' + zone + '_BGS_CIEP'] + DF['NSPL_' + zone + '_BGS_RSCP'] - DF['NSPL_' + zone + '_Total']\n",
    "    ).Check.describe().loc[['min', 'max', 'mean']])\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab46985",
   "metadata": {},
   "outputs": [],
   "source": [
    "for zone in zones_without_gen:\n",
    "\n",
    "    print(f'Checking if the sum of CIEP and RSCP BGS PLC tags sum up to the TOTAL BGS and printing the min, max and mean of the delta for zone {zone} \\n')\n",
    "\n",
    "    print(df_tags_pivoted.loc[\n",
    "        lambda DF: DF.index <= '2022-05-31'\n",
    "    ][['PLC_' + zone + '_BGS']].rename(\n",
    "        columns={\n",
    "            'PLC_' + zone + '_BGS': 'PLC_' + zone + '_BGS_CIEP'\n",
    "        }\n",
    "    ).merge(\n",
    "        right=df_rscp_tags_pivoted.pipe(\n",
    "        flatten_columns\n",
    "    )[['PLC_' + zone + '_BGS']].rename(\n",
    "        columns={\n",
    "            'PLC_' + zone + '_BGS': 'PLC_' + zone + '_BGS_RSCP'\n",
    "        }\n",
    "    ),\n",
    "    on='Date'\n",
    "    ).merge(\n",
    "        right=df_total_tags_pivoted.pipe(\n",
    "        flatten_columns\n",
    "    )[['PLC_' + zone + '_Total']],\n",
    "    on='Date'\n",
    "    ).assign(\n",
    "        Check=lambda DF: DF['PLC_' + zone + '_BGS_CIEP'] + DF['PLC_' + zone + '_BGS_RSCP'] - DF['PLC_' + zone + '_Total']\n",
    "    ).Check.describe().loc[['min', 'max', 'mean']])\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35327f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "803ffdc0",
   "metadata": {},
   "source": [
    "### Unscaling JCPL tags (and RECO NSPLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7f5f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags_pivoted_unscaled = df_tags_pivoted.merge(\n",
    "    right=df_scaling_factors_pivoted.pipe(flatten_columns),\n",
    "    on='Date'\n",
    ")\n",
    "\n",
    "df_tags_pivoted_unscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f033791",
   "metadata": {},
   "source": [
    "Unscaling BGS PLC and NSPL tags before 2017-06-01 and eligible tags before 2022-09-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17da2afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags_pivoted_unscaled['PLC_JCP&L_BGS'] = np.where(\n",
    "    df_tags_pivoted_unscaled.index < '2017-06-01',\n",
    "    df_tags_pivoted_unscaled['PLC_JCP&L_BGS'] / df_tags_pivoted_unscaled['PLC_JCP&L'],\n",
    "    df_tags_pivoted_unscaled['PLC_JCP&L_BGS']\n",
    ")\n",
    "\n",
    "df_tags_pivoted_unscaled['NSPL_JCP&L_BGS'] = np.where(\n",
    "    df_tags_pivoted_unscaled.index < '2017-06-01',\n",
    "    df_tags_pivoted_unscaled['NSPL_JCP&L_BGS'] / df_tags_pivoted_unscaled['NSPL_JCP&L'],\n",
    "    df_tags_pivoted_unscaled['NSPL_JCP&L_BGS']\n",
    ")\n",
    "\n",
    "df_tags_pivoted_unscaled['PLC_JCP&L_Eligible'] = np.where(\n",
    "    df_tags_pivoted_unscaled.index < '2022-09-01',\n",
    "    df_tags_pivoted_unscaled['PLC_JCP&L_Eligible'] / df_tags_pivoted_unscaled['PLC_JCP&L'],\n",
    "    df_tags_pivoted_unscaled['PLC_JCP&L_Eligible']\n",
    ")\n",
    "\n",
    "df_tags_pivoted_unscaled['NSPL_JCP&L_Eligible'] = np.where(\n",
    "    df_tags_pivoted_unscaled.index < '2022-09-01',\n",
    "    df_tags_pivoted_unscaled['NSPL_JCP&L_Eligible'] / df_tags_pivoted_unscaled['NSPL_JCP&L'],\n",
    "    df_tags_pivoted_unscaled['NSPL_JCP&L_Eligible']\n",
    ")\n",
    "\n",
    "df_tags_pivoted_unscaled['NSPL_RECO_BGS'] = df_tags_pivoted_unscaled['NSPL_RECO_BGS'] / df_tags_pivoted_unscaled['NSPL_RECO']\n",
    "df_tags_pivoted_unscaled['NSPL_RECO_Eligible'] = df_tags_pivoted_unscaled['NSPL_RECO_Eligible'] / df_tags_pivoted_unscaled['NSPL_RECO'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags_pivoted_unscaled.drop( # dropping the scaling factors\n",
    "    columns=['PLC_ACE', 'PLC_JCP&L', 'NSPL_JCP&L', 'PLC_PSEG', 'PLC_RECO', 'NSPL_RECO'],\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1a7065",
   "metadata": {},
   "source": [
    "### Load Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b99ee2",
   "metadata": {},
   "source": [
    "The load factor is the most important analysis on this deal. Load factor = load divided by tag. For our usual deals, we prefer high load factor customers (high load, low tag) since we get paid more for more load, and incur lower costs on lower capacity.\n",
    "\n",
    "However, CIEP is effectively a REC, Ancillary and DART deal that is paid on capacity. So low load factor customers (low load, high tag) since our REC, ancillary and DART costs will be lower while we get paid more for the UCAP.\n",
    "\n",
    "The same load factor risk exists in all deals, but is more pronounced for this deal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8edd6dc",
   "metadata": {},
   "source": [
    "#### Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80fba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating MultiIndex for load and tag for ease of calculation - should have ideally done this at the beginning\n",
    "\n",
    "df_tags_pivoted_unscaled_multi = df_tags_pivoted_unscaled.copy()\n",
    "\n",
    "df_tags_pivoted_unscaled_multi.columns = pd.MultiIndex.from_tuples([col.split('_') for col in df_tags_pivoted_unscaled.columns])\n",
    "\n",
    "df_tags_pivoted_unscaled_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb10ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping these columns as we already concluded that excess generation is not part of the pricing load and hence not used for anything\n",
    "\n",
    "df_load_pivoted_multi = df_load_pivoted.copy().drop(\n",
    "    columns=['JCP&L_Excess_Generation_BGS', 'JCP&L_Excess_Generation_Eligible', 'JCP&L_Net_BGS', 'JCP&L_Net_Eligible']\n",
    ")\n",
    "\n",
    "df_load_pivoted_multi.columns = pd.MultiIndex.from_tuples([col.split('_') for col in df_load_pivoted_multi.columns])\n",
    "\n",
    "df_load_pivoted_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e3eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the term load factors implied from our current load and capacity forecasts - this will be overlain on the charts for context\n",
    "\n",
    "chosen_lf = {\n",
    "    'ACE': 0.7876,\n",
    "    'JCP&L': 1.0191,\n",
    "    'PSEG': 0.8887,\n",
    "    'RECO': 0.5943\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe072f99",
   "metadata": {},
   "source": [
    "#### Daily load factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7930f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for zone in zones_without_gen:\n",
    "\n",
    "    fig = (df_load_pivoted_multi.reset_index().groupby(\n",
    "        [('Date', '')]\n",
    "    ).mean().drop(\n",
    "        columns=[('Hour', '')]\n",
    "    ) / df_tags_pivoted_unscaled_multi.xs(\n",
    "        'PLC', level=0, axis=1\n",
    "    )).xs(\n",
    "        zone, level=0, axis=1\n",
    "    ).pipe(\n",
    "        px.line,\n",
    "        markers=True,\n",
    "        title=f'{zone} Daily Load Factor',\n",
    "        labels={\n",
    "            'index': 'Date',\n",
    "            'value': 'Load Factor'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    fig.add_hline(y=chosen_lf[zone], line_dash='dash', line_color='black')\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971312d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for type in ['BGS', 'Eligible']:\n",
    "\n",
    "    (df_load_pivoted_multi.reset_index().groupby(\n",
    "            [('Date', '')]\n",
    "        ).mean().drop(\n",
    "            columns=[('Hour', '')]\n",
    "        ) / df_tags_pivoted_unscaled_multi.xs(\n",
    "            'PLC', level=0, axis=1\n",
    "        )).xs(\n",
    "            type, level=1, axis=1\n",
    "        ).reset_index().rename(\n",
    "            columns={\n",
    "                'index': 'Date'\n",
    "            }\n",
    "        ).melt(\n",
    "            id_vars='Date',\n",
    "            var_name='Zone',\n",
    "            value_name='Load_Factor'\n",
    "        ).assign(\n",
    "            Month=lambda DF: DF.Date.dt.month,\n",
    "            Season=lambda DF: np.where(DF.Month.isin([12, 1, 2, 3]), 'Winter', np.where(DF.Month.isin([6, 7, 8, 9]), 'Summer', 'Shoulder'))\n",
    "        ).pipe(\n",
    "                px.histogram,\n",
    "                x='Load_Factor',\n",
    "                color='Season',\n",
    "                facet_row='Zone',\n",
    "                opacity=0.75,\n",
    "                barmode='overlay',\n",
    "                height=750,\n",
    "                category_orders={'Season': ['Shoulder', 'Winter', 'Summer']},\n",
    "                title=f'{type} daily load factor histogram'\n",
    "            ).add_vline(\n",
    "                x=chosen_lf['ACE'], line_dash='dash', line_color='black', row=4\n",
    "            ).add_vline(\n",
    "                x=chosen_lf['JCP&L'], line_dash='dash', line_color='black', row=3\n",
    "            ).add_vline(\n",
    "                x=chosen_lf['PSEG'], line_dash='dash', line_color='black', row=2\n",
    "            ).add_vline(\n",
    "                x=chosen_lf['RECO'], line_dash='dash', line_color='black', row=1\n",
    "            ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e366f91",
   "metadata": {},
   "source": [
    "#### Monthly load factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d63a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_load_factor = (df_load_pivoted_multi.reset_index().assign(\n",
    "        Month_start=lambda DF: DF[('Date', '')].dt.to_period('M').dt.to_timestamp()\n",
    "    ).groupby(\n",
    "        [('Month_start', '')]\n",
    "    ).mean().drop(\n",
    "        columns=[\n",
    "            ('Date', ''),\n",
    "            ('Hour', '')\n",
    "        ]\n",
    "    ) / df_tags_pivoted_unscaled_multi.reset_index().assign(\n",
    "        Month_start=lambda DF: DF[('Date', '', '')].dt.to_period('M').dt.to_timestamp()\n",
    "    ).groupby(\n",
    "        [('Month_start', '', '')]\n",
    "    ).mean().drop(\n",
    "        columns=[\n",
    "            ('Date', '', '')\n",
    "        ]\n",
    "    ).xs(\n",
    "        'PLC', level=0, axis=1\n",
    "    ).rename_axis([\n",
    "        ('Month_start', '')\n",
    "    ]))\n",
    "\n",
    "for zone in zones_without_gen:\n",
    "\n",
    "    fig = monthly_load_factor.reset_index().pipe(\n",
    "        flatten_columns\n",
    "    ).pipe(\n",
    "        px.line,\n",
    "        x='Month_start',\n",
    "        y=[zone + '_BGS', zone + '_Eligible'],\n",
    "        markers=True,\n",
    "        title=f'{zone} Monthly Load Factor',\n",
    "        labels={\n",
    "            'value': 'Load Factor'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    fig.add_hline(\n",
    "        y=chosen_lf[zone],\n",
    "        line_dash='dash',\n",
    "        line_color='black'\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048854d4",
   "metadata": {},
   "source": [
    "#### Monthly load factor box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951c5713",
   "metadata": {},
   "outputs": [],
   "source": [
    "for zone in zones_without_gen:\n",
    "\n",
    "    fig = monthly_load_factor.xs(\n",
    "        'BGS', level=1, axis=1\n",
    "    ).reset_index().rename(\n",
    "        columns={\n",
    "            ('Month_start', '') : 'Month_start'\n",
    "        }\n",
    "    ).assign(\n",
    "        PY=lambda DF: DF.Month_start.apply(planning_year)\n",
    "    ).pipe(\n",
    "        px.box,\n",
    "        x='PY',\n",
    "        y=zone,\n",
    "        points='all',\n",
    "        title=f'{zone} monthly load factor distribution'\n",
    "    )\n",
    "\n",
    "    fig.add_hline(\n",
    "        y=chosen_lf[zone],\n",
    "        line_dash='dash',\n",
    "        line_color='red'\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df04227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_load_factor.xs(\n",
    "        'BGS', level=1, axis=1\n",
    "    ).reset_index().rename(\n",
    "        columns={\n",
    "            ('Month_start', '') : 'Month_start'\n",
    "        }\n",
    "    ).assign(\n",
    "        PY=lambda DF: DF.Month_start.apply(planning_year)\n",
    "    ).drop(\n",
    "        columns='Month_start'\n",
    "    ).groupby(\n",
    "        'PY'\n",
    "    ).mean().reset_index().melt(\n",
    "        id_vars='PY',\n",
    "        value_vars=zones_without_gen,\n",
    "        var_name='Zone',\n",
    "        value_name='Load Factor'\n",
    "    ).pipe(\n",
    "        px.line,\n",
    "        x='PY',\n",
    "        y='Load Factor',\n",
    "        facet_row='Zone',\n",
    "        markers='True',\n",
    "        height=800,\n",
    "        title='Average load factor by planning year'\n",
    "    ).add_hline(\n",
    "        y=chosen_lf['ACE'], line_dash='dash', line_color='red', row=4\n",
    "    ).add_hline(\n",
    "        y=chosen_lf['JCP&L'], line_dash='dash', line_color='red', row=3\n",
    "    ).add_hline(\n",
    "        y=chosen_lf['PSEG'], line_dash='dash', line_color='red', row=2\n",
    "    ).add_hline(\n",
    "        y=chosen_lf['RECO'], line_dash='dash', line_color='red', row=1\n",
    "    ).update_yaxes(matches=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0409f12d",
   "metadata": {},
   "source": [
    "#### Plotting load and tag separately as a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07938de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_load_by_tags(DF, zones):\n",
    "    for zone in zones:\n",
    "        DF[('Load_Factor', zone)] = DF[('Load', zone)] / DF[('Tags', zone)]\n",
    "    return DF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49f985a",
   "metadata": {},
   "source": [
    "The long pipe in the next cell is just for merging the load and tags so that the load factor can be plotted in the same chart as load and tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6925a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for zone in zones_without_gen:\n",
    "\n",
    "    df_load_pivoted_multi.reset_index().assign(\n",
    "        Month_start=lambda DF: DF[('Date', '')].dt.to_period('M').dt.to_timestamp()\n",
    "    ).groupby(\n",
    "        [('Month_start', '')]\n",
    "    ).mean().drop(\n",
    "        columns=[\n",
    "            ('Date', ''),\n",
    "            ('Hour', '')\n",
    "        ]\n",
    "    ).xs(\n",
    "        'BGS', level=1, axis=1\n",
    "    ).reset_index().rename(\n",
    "        columns={\n",
    "            ('Month_start', ''): 'Month_start'\n",
    "        }\n",
    "    ).set_index('Month_start').set_axis(\n",
    "        pd.MultiIndex.from_product(\n",
    "        [\n",
    "            ['Load'], zones_without_gen\n",
    "        ]\n",
    "    ), axis=1  \n",
    "        ).merge(\n",
    "    df_tags_pivoted_unscaled_multi.reset_index().assign(\n",
    "            Month_start=lambda DF: DF[('Date', '', '')].dt.to_period('M').dt.to_timestamp()\n",
    "        ).groupby(\n",
    "            [('Month_start', '', '')]\n",
    "        ).mean().drop(\n",
    "            columns=[\n",
    "                ('Date', '', '')\n",
    "            ]\n",
    "        ).xs(\n",
    "            'PLC', level=0, axis=1\n",
    "        ).xs(\n",
    "            'BGS', level=1, axis=1\n",
    "        ).reset_index().rename(\n",
    "            columns={\n",
    "            ('Month_start', '', ''): 'Month_start' \n",
    "            }\n",
    "        ).set_index('Month_start').set_axis(\n",
    "                pd.MultiIndex.from_product(\n",
    "        [\n",
    "            ['Tags'], zones_without_gen\n",
    "        ]\n",
    "    ), axis=1  \n",
    "        ),\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        ).pipe(\n",
    "            divide_load_by_tags, zones_without_gen\n",
    "        ).xs(\n",
    "            zone, level=1, axis=1\n",
    "        ).reset_index().melt(\n",
    "            id_vars='Month_start',\n",
    "            value_vars=['Load', 'Tags', 'Load_Factor']\n",
    "        ).pipe(\n",
    "            px.line,\n",
    "            x='Month_start',\n",
    "            y='value',\n",
    "            facet_row='variable',\n",
    "            height=750,\n",
    "            title=f'{zone} monthly load, tag and load factor',\n",
    "            markers=True\n",
    "        ).update_yaxes(matches=None).for_each_annotation(\n",
    "            lambda txt: txt.update(text=txt.text.split(\"=\")[-1])\n",
    "        ).add_hline(\n",
    "            y=chosen_lf[zone], line_dash='dash', line_color='red', row=1\n",
    "        ).show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2513f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b1329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c7fc98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53499ddc",
   "metadata": {},
   "source": [
    "#### Plotting historical CP charts for capacity (using BIG migration-adjusted load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3911ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_dates = pd.read_excel(\n",
    "    r'K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\Capacity\\Capacity FORECAST ALL - CURRENT.xlsx',\n",
    "    sheet_name='Zonal Peaks',\n",
    "    usecols=range(10, 22),\n",
    "    skiprows=2,\n",
    "    nrows=6\n",
    ")\n",
    "\n",
    "peak_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8cffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_hours = pd.read_excel(\n",
    "    r'K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\Capacity\\Capacity FORECAST ALL - CURRENT.xlsx',\n",
    "    sheet_name='Zonal Peaks',\n",
    "    usecols=range(10, 22),\n",
    "    skiprows=8,\n",
    "    nrows=6 \n",
    ")\n",
    "\n",
    "peak_hours.columns = peak_dates.columns\n",
    "\n",
    "peak_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552d8412",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_date_hours = (peak_dates.astype(str) + ' ' + (peak_hours - 1).astype(str) + ':00:00').apply(pd.to_datetime)\n",
    "\n",
    "peak_date_hours = peak_date_hours.melt().rename(\n",
    "    columns={\n",
    "        'variable': 'PY',\n",
    "        'value': 'DateTime'\n",
    "    }\n",
    ")\n",
    "\n",
    "peak_date_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e5e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_template_path = r'K:\\Valuation\\Wholesale for Retail\\_Full Requirements\\PJM\\BGS FP & CIEP\\2026-02\\CIEP\\Pri Hemanth\\Load\\Pri Hemanth BGS-CIEP 2026-02 Load Template Pro v2.1.xlsm'\n",
    "\n",
    "for sheet_name in [\n",
    "    'AECO_ACE_BGS',\n",
    "    'JCPL_JCPL_BGS',\n",
    "    'PSEG_PSEG_BGS',\n",
    "    'RECO_RECO_BGS'\n",
    "]:\n",
    "\n",
    "    df_temp = pd.read_excel(\n",
    "        load_template_path,\n",
    "        sheet_name=sheet_name,\n",
    "        usecols=[1, 2, 6],\n",
    "        skiprows=6\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'Big.1': 'Load (MWh)'\n",
    "        }\n",
    "    ).assign(\n",
    "        # DateTime=lambda DF: DF.Date + pd.to_timedelta(DF.HE - 1, unit='h'),\n",
    "        Year=lambda DF: DF.Date.dt.year\n",
    "    )\n",
    "\n",
    "    fig = peak_date_hours.assign(\n",
    "    Date=lambda DF: pd.to_datetime(DF.DateTime.dt.date)\n",
    "    ).merge(\n",
    "        df_temp,\n",
    "        on='Date'\n",
    "    ).pipe(\n",
    "        px.line,\n",
    "        x='HE',\n",
    "        y='Load (MWh)',\n",
    "        facet_col='Year',\n",
    "        facet_col_wrap=4,\n",
    "        color='Date',\n",
    "        title=f'{sheet_name.split('_')[0]} 5CP Days'\n",
    "        # markers=True\n",
    "    )\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    years = [2022, 2023, 2024, 2025, 2018, 2019, 2020, 2021, 2014, 2015, 2016, 2017] # Setting this to be the order since plotly sets row 1, col 1 to be the the bottom left\n",
    "\n",
    "    for row in range(3): # 12 years in total = 3 * 4\n",
    "        for col in range(4):\n",
    "\n",
    "            peaks = peak_date_hours.assign(\n",
    "                    Date=lambda DF: pd.to_datetime(DF.DateTime.dt.date)\n",
    "                    ).merge(\n",
    "                        df_temp,\n",
    "                        on='Date'\n",
    "                    ).assign(\n",
    "                        DateTime_Check=lambda DF: DF.Date + pd.to_timedelta(DF.HE, unit='h')\n",
    "                    ).loc[\n",
    "                        lambda DF: DF.DateTime == DF.DateTime_Check\n",
    "                    ][['HE', 'Load (MWh)', 'Year']].loc[\n",
    "                        lambda DF: DF.Year == years[count]\n",
    "                    ]\n",
    "            \n",
    "            cp_peak_hours = peaks['HE']\n",
    "            cp_peak_load = peaks['Load (MWh)']\n",
    "\n",
    "            fig.add_scatter(\n",
    "                x=cp_peak_hours,\n",
    "                y=cp_peak_load,\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=7,\n",
    "                    color='black'\n",
    "                ),\n",
    "                row=row + 1,\n",
    "                col=col + 1\n",
    "            )\n",
    "\n",
    "            count += 1\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b96c894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e3eca46",
   "metadata": {},
   "source": [
    "### DART spread analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633647de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dart_zones = {\n",
    "    'AECO': 116472927,\n",
    "    'JCPL': 116472945,\n",
    "    'PSEG': 116472957,\n",
    "    'RECO': 116472959\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c059b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dart_zone in dart_zones.keys():\n",
    "\n",
    "    fig = pull_lmp_data(\n",
    "        emtdb=emtdb,\n",
    "        pnode_id=dart_zones[dart_zone],\n",
    "        da_or_rt='DA',\n",
    "        start_dt='2014-01-01',\n",
    "        end_dt=pd.Timestamp.today().date() - pd.offsets.MonthEnd(),\n",
    "        price_data_type='PRICE'\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'Price': 'DA'\n",
    "        }\n",
    "    ).reset_index().merge(pull_lmp_data(\n",
    "        emtdb=emtdb,\n",
    "        pnode_id=dart_zones[dart_zone],\n",
    "        da_or_rt='RT',\n",
    "        start_dt='2014-01-01',\n",
    "        end_dt=pd.Timestamp.today().date() - pd.offsets.MonthEnd(),\n",
    "        price_data_type='PRICE'\n",
    "    ).reset_index().rename(\n",
    "        columns={\n",
    "            'Price': 'RT'\n",
    "        }\n",
    "    ), on=['Date', 'Hour']).assign(\n",
    "        DART=lambda DF: DF.DA - DF.RT\n",
    "    ).pipe(\n",
    "        px.line,\n",
    "        x='Date',\n",
    "        y='DART',\n",
    "        title=f'{dart_zone} DART spread (DA minus RT)',\n",
    "        labels={\n",
    "            'DART': 'DART ($/MWh)'\n",
    "        }\n",
    "    ).update_yaxes(range=[-500, 500])\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee010fdf",
   "metadata": {},
   "source": [
    "Ideally, for pricing an energy-pass through deal where we're paid on RT and incur costs on DA should have an covariance that is based on the DART spread. However, I think the expected value of the DART spread is close to 0 (checked below for PSEG), leading to a small covariance in practice. Hence we don't calculate the LFA on this deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683255f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_lmp_data(\n",
    "        emtdb=emtdb,\n",
    "        pnode_id=dart_zones['PSEG'],\n",
    "        da_or_rt='DA',\n",
    "        start_dt='2014-01-01',\n",
    "        end_dt=pd.Timestamp.today().date() - pd.offsets.MonthEnd(),\n",
    "        price_data_type='PRICE'\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'Price': 'DA'\n",
    "        }\n",
    "    ).reset_index().merge(pull_lmp_data(\n",
    "        emtdb=emtdb,\n",
    "        pnode_id=dart_zones['PSEG'],\n",
    "        da_or_rt='RT',\n",
    "        start_dt='2014-01-01',\n",
    "        end_dt=pd.Timestamp.today().date() - pd.offsets.MonthEnd(),\n",
    "        price_data_type='PRICE'\n",
    "    ).reset_index().rename(\n",
    "        columns={\n",
    "            'Price': 'RT'\n",
    "        }\n",
    "    ), on=['Date', 'Hour']).assign(\n",
    "        DART=lambda DF: DF.DA - DF.RT\n",
    "    ).merge(\n",
    "        df_load_pivoted[['PSEG_BGS']].reset_index(),\n",
    "        on=['Date', 'Hour']\n",
    "    ).pipe(\n",
    "        # lambda DF: DF['DART'].mean(), # Checking expectation of DART\n",
    "        lambda DF: DF['DART'].cov(DF['PSEG_BGS']) # Checking covariance of DART with load\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04d6b48",
   "metadata": {},
   "source": [
    "#### Calculating DART hub-basis correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c70b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dart_zone in dart_zones.keys():\n",
    "\n",
    "    fig = pull_lmp_data(\n",
    "            emtdb=emtdb,\n",
    "            pnode_id=dart_zones[dart_zone],\n",
    "            da_or_rt='DA',\n",
    "            start_dt='2014-01-01',\n",
    "            end_dt=pd.Timestamp.today().date() - pd.offsets.MonthEnd(),\n",
    "            price_data_type='PRICE'\n",
    "        ).rename(\n",
    "            columns={\n",
    "                'Price': 'DA_zone'\n",
    "            }\n",
    "        ).reset_index().merge(pull_lmp_data(\n",
    "            emtdb=emtdb,\n",
    "            pnode_id=dart_zones[dart_zone],\n",
    "            da_or_rt='RT',\n",
    "            start_dt='2014-01-01',\n",
    "            end_dt=pd.Timestamp.today().date() - pd.offsets.MonthEnd(),\n",
    "            price_data_type='PRICE'\n",
    "        ).reset_index().rename(\n",
    "            columns={\n",
    "                'Price': 'RT_zone'\n",
    "            }\n",
    "        ), on=['Date', 'Hour']).assign(\n",
    "            DART_zone=lambda DF: DF.DA_zone - DF.RT_zone\n",
    "        ).merge(pull_lmp_data(\n",
    "            emtdb=emtdb,\n",
    "            pnode_id=51288, # PJM-W\n",
    "            da_or_rt='DA',\n",
    "            start_dt='2014-01-01',\n",
    "            end_dt=pd.Timestamp.today().date() - pd.offsets.MonthEnd(),\n",
    "            price_data_type='PRICE'\n",
    "        ).reset_index().rename(\n",
    "            columns={\n",
    "                'Price': 'DA_hub'\n",
    "            }\n",
    "        ), on=['Date', 'Hour']).merge(pull_lmp_data(\n",
    "            emtdb=emtdb,\n",
    "            pnode_id=51288, # PJM-W\n",
    "            da_or_rt='RT',\n",
    "            start_dt='2014-01-01',\n",
    "            end_dt=pd.Timestamp.today().date() - pd.offsets.MonthEnd(),\n",
    "            price_data_type='PRICE'\n",
    "        ).reset_index().rename(\n",
    "            columns={\n",
    "                'Price': 'RT_hub'\n",
    "            }\n",
    "        ), on=['Date', 'Hour']).assign(\n",
    "            DART_hub=lambda DF: DF.DA_hub - DF.RT_hub,\n",
    "            DART_basis=lambda DF: DF.DART_zone - DF.DART_hub,\n",
    "            Month=lambda DF: DF.Date.dt.month,\n",
    "            Year=lambda DF: DF.Date.dt.year\n",
    "        ).groupby(['Year', 'Month']).apply(\n",
    "            lambda DF: DF.DART_basis.corr(DF.DART_hub)\n",
    "        ).reset_index().pivot(\n",
    "            index='Month',\n",
    "            columns='Year',\n",
    "            values=0\n",
    "        ).pipe(\n",
    "            px.imshow,\n",
    "            title=f'{dart_zone} DART hub-basis correlations',\n",
    "            # width=10,\n",
    "            # height=10\n",
    "        )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50577344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "555c6af6",
   "metadata": {},
   "source": [
    "### NJ REC Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb9351d",
   "metadata": {},
   "source": [
    "Array.from(document.querySelectorAll('a[href*=\".xlsx\"], a[href*=\".xls\"]'))\n",
    "  .map(a => a.href)\n",
    "  .forEach(url => console.log(url));\n",
    "\n",
    "I used the above block of JS code to get the urls from https://njcleanenergy.com/renewable-energy/project-activity-reports/solar-activity-report-archive, copied them manually into an excel file and then downloaded them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a604f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Paste all the URLs you got from the browser here\n",
    "\n",
    "urls = pd.read_excel(\n",
    "    r'K:\\Valuation\\Wholesale for Retail\\_Full Requirements\\PJM\\BGS FP & CIEP\\2026-02\\CIEP\\Pri Hemanth\\Output and Analysis\\RECs\\Solar Activity URLs.xlsx'\n",
    ").iloc[:, 0].tolist()\n",
    "\n",
    "download_dir = Path(\"nj_solar_reports\")\n",
    "download_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for idx, url in enumerate(urls, 1):\n",
    "    try:\n",
    "        filename = url.split('/')[-1].replace('+', '_')\n",
    "        filepath = download_dir / filename\n",
    "        \n",
    "        print(f\"[{idx}/{len(urls)}] Downloading {filename}...\")\n",
    "        response = requests.get(url, stream=True, timeout=30)\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            for chunk in response.iter_content(8192):\n",
    "                f.write(chunk)\n",
    "        \n",
    "        print(f\"  ✓ Saved\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\")\n",
    "\n",
    "print(f\"\\nDone! Files in {download_dir.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c2a0e6",
   "metadata": {},
   "source": [
    "### Working Capital Slides Automation - did not work in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fe06d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:\\Valuation\\Wholesale for Retail\\_Full Requirements\\PJM\\BGS FP & CIEP\\2026-02\\CIEP\\Pri Hemanth\\Output and Analysis\\Working Capital\\Pri Hemanth BGS-CIEP 2026-02 PJM WC Model V2.0 - ACE Scenario 1.xlsm\n",
      "K:\\Valuation\\Wholesale for Retail\\_Full Requirements\\PJM\\BGS FP & CIEP\\2026-02\\CIEP\\Pri Hemanth\\Output and Analysis\\Working Capital\\Pri Hemanth BGS-CIEP 2026-02 PJM WC Model V2.0 - ACE Scenario 2.xlsm\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# If this fails, try closing Excel in task manager - foreground and background processes\n",
    "\n",
    "folder_path = r'K:\\Valuation\\Wholesale for Retail\\_Full Requirements\\PJM\\BGS FP & CIEP\\2026-02\\CIEP\\Pri Hemanth\\Output and Analysis\\Working Capital'\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if 'Scenario' in file_name:\n",
    "    \n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        print(file_path)\n",
    "\n",
    "        pythoncom.CoInitialize()\n",
    "        try:\n",
    "            # Open Excel\n",
    "            excel = win32com.client.Dispatch(\"Excel.Application\")\n",
    "            excel.Visible = False  # Set to True if you want to see Excel\n",
    "            \n",
    "            # Open your workbook\n",
    "            workbook = excel.Workbooks.Open(file_path)\n",
    "            \n",
    "            # Run the macro (use the macro name as it appears in VBA)\n",
    "            excel.Application.Run(\"CreatePPT\")\n",
    "            \n",
    "            # Save and close\n",
    "            workbook.Save()\n",
    "            workbook.Close()\n",
    "            excel.Quit()\n",
    "            \n",
    "        finally:\n",
    "            # Uninitialize COM\n",
    "            pythoncom.CoUninitialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bdd2d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a32a9e3",
   "metadata": {},
   "source": [
    "### Exporting to Load Agg Excel file (while archiving if it already exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1275d040",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_agg_folder_path = r'K:\\Valuation\\Wholesale for Retail\\_Full Requirements\\PJM\\BGS FP & CIEP\\2026-02\\CIEP\\Pri Hemanth\\Load'\n",
    "load_agg_archived_folder_path = r'K:\\Valuation\\Wholesale for Retail\\_Full Requirements\\PJM\\BGS FP & CIEP\\2026-02\\CIEP\\Pri Hemanth\\Load\\Archived'\n",
    "load_agg_file_name = 'Pri Hemanth BGS-CIEP 2026-02 Load Agg.xlsx'\n",
    "\n",
    "load_agg_file_path = os.path.join(load_agg_folder_path, load_agg_file_name)\n",
    "load_agg_archived_file_path = os.path.join(load_agg_archived_folder_path, load_agg_file_name)\n",
    "\n",
    "if os.path.isfile(load_agg_file_path):\n",
    "    os.makedirs(load_agg_archived_folder_path, exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(load_agg_archived_file_path):\n",
    "        print(f'Deleting old archived load agg...')\n",
    "        os.remove(load_agg_archived_file_path)\n",
    "\n",
    "    # os.rename(load_agg_file_path, load_agg_archived_file_path)\n",
    "    print(f'Copying existing load agg to archived folder and removing the version in main folder...')\n",
    "    shutil.copy2(load_agg_file_path, load_agg_archived_file_path)\n",
    "    os.remove(load_agg_file_path)\n",
    "\n",
    "with pd.ExcelWriter(load_agg_file_path, engine='openpyxl') as writer:\n",
    "    print(f'Saving new load agg...')\n",
    "    df_load_pivoted.to_excel(writer, sheet_name='Load'),\n",
    "    df_tags_pivoted_unscaled.to_excel(writer, sheet_name='Raw Tags'), # All of these are unscaled\n",
    "    df_deration_factors_pivoted.to_excel(writer, sheet_name='Deration Factors'),\n",
    "    df_scaling_factors_pivoted.to_excel(writer, sheet_name='Scaling Factors')\n",
    "    df_counts_pivoted.to_excel(writer, sheet_name='Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b445b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c72d15ae",
   "metadata": {},
   "source": [
    "## Auction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa33e4a2",
   "metadata": {},
   "source": [
    "### Decrement formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fef501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta(gamma, tranche_target: int, regime: int):\n",
    "    if regime == 1:\n",
    "        \n",
    "        if tranche_target >= 20:\n",
    "            bins = [0.13, 0.28, 0.54, 0.775]\n",
    "            values = [0.005, 0.0175, 0.03, 0.04, 0.05]\n",
    "        \n",
    "        elif 10 <= tranche_target <= 19:\n",
    "            bins = [0.13, 0.21, 0.21, 0.28]\n",
    "            values = [0.005, 0.0175, 0.03, 0.04, 0.05]\n",
    "\n",
    "        elif 2 <= tranche_target <= 9:\n",
    "            bins = [0.32, 0.55]\n",
    "            values = [0.0175, 0.03, 0.05]\n",
    "                        \n",
    "        else: # 1 tranche\n",
    "            bins = [0.2]\n",
    "            values=[0.03, 0.05]\n",
    "    \n",
    "    elif regime == 2:\n",
    "        \n",
    "        if tranche_target >= 20:\n",
    "            bins = [0.18, 0.34, 0.57, 0.75]\n",
    "            values = [0.00375, 0.0125, 0.0225, 0.03, 0.0375]\n",
    "\n",
    "        elif 10 <= tranche_target <= 19:\n",
    "            bins = [0.18, 0.28, 0.38, 0.48]\n",
    "            values = [0.00375, 0.0125, 0.0225, 0.03, 0.0375]\n",
    "\n",
    "        elif 2 <= tranche_target <= 9:\n",
    "            bins = [0.32, 0.55]\n",
    "            values = [0.0125, 0.0225, 0.0375]\n",
    "\n",
    "        else: # 1 tranche\n",
    "            bins = [0.2]\n",
    "            values=[0.0225, 0.0375] \n",
    "    \n",
    "    else: # regime == 3\n",
    "        \n",
    "        if tranche_target >= 20:\n",
    "            bins = [0.25, 0.75]\n",
    "            values = [0.0025, 0.015, 0.025]\n",
    "\n",
    "        elif 10 <= tranche_target <= 19:\n",
    "            bins = [0.25, 0.6]\n",
    "            values = [0.0025, 0.015, 0.025]\n",
    "\n",
    "        elif 2 <= tranche_target <= 9:\n",
    "            bins = [0.42]\n",
    "            values=[0.015, 0.025] \n",
    "\n",
    "        else: # 1 tranche\n",
    "            bins = [0.2]\n",
    "            values=[0.015, 0.025] \n",
    "    \n",
    "    indices = np.digitize(gamma, bins)\n",
    "    return np.array(values)[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347bf8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = np.linspace(0, 1, 1000)\n",
    "tranche_target = 24\n",
    "regime = 2\n",
    "\n",
    "px.line(\n",
    "    x=gamma, \n",
    "    y=delta(gamma, tranche_target, regime),\n",
    "    title=f'Regime {regime}, Tranche Target {tranche_target}',\n",
    "    labels={\n",
    "        'x': 'Gamma (Oversupply ratio)',\n",
    "        'y': 'Delta (Decrement)'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac33c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816e77ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
