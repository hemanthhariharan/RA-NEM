{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b23f225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly_express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import sys\n",
    "import wquantiles as wq\n",
    "from getpass import getpass\n",
    "from pyra.date_utils import get_planning_year_dates_between"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417158ae",
   "metadata": {},
   "source": [
    "## EMTDB Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966da858",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(r'K:\\Valuation\\_Analysts\\Hemanth\\Python Notebooks\\Miscellaneous\\Python Analyst Engine 2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29cb25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import EmtdbConnection\n",
    "from emtdb_api import pull_lmp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1746272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'HXH07BP'\n",
    "pw = getpass('Enter EMTDB pass:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd51b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "emtdb = EmtdbConnection(user, pw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a5326",
   "metadata": {},
   "source": [
    "## Analyst Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750973c9",
   "metadata": {},
   "source": [
    "### Standard inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee1e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_planning_year = '2025-2026' # For annual auction. Makes sure the format is always YYYY-YYYY\n",
    "\n",
    "look_ahead_years = 4 # including the current planning year\n",
    "\n",
    "zone = 'DPL' # Depending on deal\n",
    "\n",
    "# zone, sink_ID and sink_name:\n",
    "# AECO, 51291, AECO\n",
    "# JCPL, 116472945, JCPL_RESID_AGG\n",
    "# PSEG, 51301, PSEG\n",
    "# RECO, 116472959, RECO_RESID_AGG\n",
    "# PEPCO, 338268, PEPCO DC\n",
    "# APS, 116472931, APS_RESID_AGG\n",
    "# METED, 51296, METED\n",
    "# PENELEC, 116472951, PENELEC_RESID_AGG\n",
    "# METED, 51295, METED\n",
    "# ATSI, 1258625176, FEOH\n",
    "# UGI, 116472955, UGI_RESID_AGG\n",
    "# DPL, 51293, DPL\n",
    "\n",
    "sink_id = 51293 # read from K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\ARRs\\ARR Path Selection\\2025\\Hemanth\\Data\\Sink Node ID.xlsx\n",
    "\n",
    "sink_name = 'DPL'\n",
    "\n",
    "most_recent_LT_auction = '2026-2029'\n",
    "most_recent_LT_auction_round = '4' # This should be set as a string\n",
    "\n",
    "selection_threshold = 1500 # Usually 1500\n",
    "\n",
    "percent_for_ARR = 1\n",
    "\n",
    "percent_for_stage_1A = 0.6 # 60% "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9688ccf9",
   "metadata": {},
   "source": [
    "### File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9712957",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline_path = r'K:\\Valuation\\_Analysts\\Hemanth\\ARRs\\Timeline\\2025-ftr-arr-market-schedule.xlsx'\n",
    "long_term_ftr_results_path = r'K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\ARRs\\ARR Path Selection\\2025\\Hemanth\\Data\\Long-term FTR Auction Results'\n",
    "\n",
    "annual_ftr_results_path = r'K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\ARRs\\ARR Path Selection\\2025\\Hemanth\\Data\\Annual FTR Auction Results'\n",
    "\n",
    "arr_selection_path = r'K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\ARRs\\ARR Path Selection\\2025\\Hemanth\\Data\\ARR Path Selection'\n",
    "stage_1_resources_path = r'K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\ARRs\\ARR Path Selection\\2025\\Hemanth\\Data\\Resources\\2025-26\\2025-2026-stage-1-resources-by-zone.xlsx'\n",
    "\n",
    "zonal_nspl_path = r'K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\ARRs\\PY 2526 ARR Forecast.xlsx'\n",
    "nee_nspl_path = r'K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\ARRs\\ARR Path Selection\\2025\\Hemanth\\Data\\Subaccount\\Subaccounts PY25-26 with NSPL & Paths.xlsx'\n",
    "\n",
    "stage_2_dollar_per_MW_PY_26_27_path = r'K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\ARRs\\ARR Path Selection\\2025\\Betty\\Forecast\\Stage 2 MT\\PY 2627 Stage 2 ARR - updated.xlsx' \n",
    "stage_2_dollar_per_MW_PY_27_28_path = r'K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\ARRs\\ARR Path Selection\\2025\\Betty\\Forecast\\Stage 2 MT\\PY 2728 Stage 2 ARR - updated.xlsx'  \n",
    "\n",
    "stage_1B_path = r'K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\ARRs\\ARR Path Selection\\2025\\Hemanth\\Data\\ARR Path Selection\\PJM Stage 1B ARR Results 25-26.xlsx'\n",
    "\n",
    "stage_2_path_r1 = r'K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\ARRs\\ARR Path Selection\\2025\\Hemanth\\Data\\ARR Path Selection\\PJM S2R1 Pull 20250326 (awards).xlsx'\n",
    "stage_2_path_r2 = r'K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\ARRs\\ARR Path Selection\\2025\\Hemanth\\Data\\ARR Path Selection\\PJM S2R2 Pull 20250403.xlsx'\n",
    "\n",
    "stage_1A_pull_path = r'K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\ARRs\\ARR Path Selection\\2025\\Hemanth\\Data\\ARR Path Selection\\PJM 1A Pull 20250603.xlsx' # For stage 2\n",
    "\n",
    "zonal_arr_path = r'K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\ARRs\\PJM ARR Tracking.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec86bd3b",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db207978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_planning_year(current_planning_year): # returns next planning year given the current planning year\n",
    "    return str(int(current_planning_year[:4]) + 1) + '-' + str(int(current_planning_year[5:]) + 1)\n",
    "\n",
    "def planning_year_from_LT_auction(DF): # returns the planning year corresponding to the year of the long-term auction\n",
    "    return str(int(DF['LT_Period'][:4]) + int(DF['Period Type'][-1]) - 1) + '-' + str(int(DF['LT_Period'][:4]) + int(DF['Period Type'][-1]))\n",
    "    \n",
    "def planning_year_from_date(DF): # returns the planning year corresponding to a date\n",
    "    if (DF['Date'].month >= 6):\n",
    "        return str(int(DF['Date'].year)) + '-' + str(int(DF['Date'].year) + 1)\n",
    "    else:\n",
    "        return str(int(DF['Date'].year) - 1) + '-' + str(int(DF['Date'].year))\n",
    "\n",
    "\n",
    "next_year = next_planning_year(current_planning_year)\n",
    "next_to_next_year = next_planning_year(next_year)\n",
    "next_to_next_to_next_year = next_planning_year(next_to_next_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflatten_columns(DF):\n",
    "    temp_DF = DF.copy()\n",
    "    temp_DF.columns = pd.MultiIndex.from_tuples(\n",
    "    [tuple(col.split('_')) for col in temp_DF.columns]\n",
    ")\n",
    "    \n",
    "    return temp_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75062cb7",
   "metadata": {},
   "source": [
    "## Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d359ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeline = pd.read_excel(timeline_path)\n",
    "\n",
    "df_timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89fb1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.timeline(df_timeline.dropna(), x_start='Start Day', x_end='End Day', y='Market Name', title='ARR/FTR Auction Period', facet_col='Product')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c84a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.timeline(df_timeline.dropna(), x_start='Bidding Opening Day', x_end='Bidding Closing Day', y='Market Name', title='ARR/FTR Auction Dates', facet_col='Product')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0035f62f",
   "metadata": {},
   "source": [
    "## Stage 1 Resources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4617ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stage_1_resources = pd.concat(\n",
    "    pd.read_excel(\n",
    "    stage_1_resources_path,\n",
    "    sheet_name=None, # Reading all sheets and combining into single df\n",
    "    header=[0, 1]\n",
    ").values(),\n",
    "ignore_index=True\n",
    ").dropna(\n",
    "    how='all', axis=0 # Dropping rows with no values\n",
    ").iloc[:, [0, 1, 2, 5, 6, 10]]\n",
    "\n",
    "df_stage_1_resources.columns = ['Zone', 'Pnode ID', 'FTR Name', current_planning_year+'_Capacity MW', 'Retired', 'Rate_based']\n",
    "\n",
    "df_stage_1_resources.rename(\n",
    "    columns={'Pnode ID': 'PNODEID'}, inplace=True\n",
    ")\n",
    "\n",
    "df_stage_1_resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89b6648",
   "metadata": {},
   "source": [
    "## Annual FTR Auction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5864e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annual_ftr_results = pd.DataFrame()\n",
    "\n",
    "for file_name in os.listdir(os.path.join(annual_ftr_results_path, current_planning_year)):\n",
    "    file_path = os.path.join(os.path.join(annual_ftr_results_path, current_planning_year), file_name)\n",
    "    if 'round' in file_path:\n",
    "        print(f'Aggregating annual FTR results from {file_name}')\n",
    "        df_annual_ftr_results_temp = pd.read_excel(file_path, sheet_name='Obligation Nodal Prices RD ' + file_path[-14]) # file_path[-14] gets the round number from the file name\n",
    "        df_annual_ftr_results_temp['Annual_Round'] = file_path[-14]\n",
    "        df_annual_ftr_results = pd.concat([df_annual_ftr_results, df_annual_ftr_results_temp]) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa34fcf",
   "metadata": {},
   "source": [
    "Checking where missing values are present and dropping rows and columns not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annual_ftr_results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_annual_ftr_results[df_annual_ftr_results.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdd8992",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annual_ftr_results.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c12e951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns not needed\n",
    "\n",
    "df_annual_ftr_results = df_annual_ftr_results.iloc[:, [0, 1, 3, 8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdb85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annual_ftr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954c7043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivoting the dataframe and calculating the average of the four rounds \n",
    "\n",
    "df_annual_ftr_results = df_annual_ftr_results.pivot(\n",
    "    index=['Node', 'PNODEID'],\n",
    "    columns='Annual_Round',\n",
    "    values='LMP',\n",
    ").reset_index()\n",
    "\n",
    "df_annual_ftr_results['Average_LMP'] = df_annual_ftr_results.iloc[:, 2:].mean(axis=1) # These are the average LMPs of the sources\n",
    "\n",
    "df_annual_ftr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99949fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_annual_ftr_results.loc[\n",
    "#     lambda DF: (DF.Node.isin(['RTEP B0287 SOURCE','RTEP B0328 SOURCE', 'DPL', 'PEPCO']))\n",
    "# ].to_clipboard(index=False)\n",
    "\n",
    "\n",
    "df_annual_ftr_results.loc[\n",
    "    lambda DF: DF.Node.isin(pd.read_clipboard().SourceName) | DF.Node.isin(pd.read_clipboard().SinkName)\n",
    "].to_clipboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6d48d7",
   "metadata": {},
   "source": [
    "## Long-term FTR auction results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b15c33f",
   "metadata": {},
   "source": [
    "Iterating through different LT auction folders and the files in each folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89114cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_term_ftr_results = pd.DataFrame()\n",
    "\n",
    "for folder_name in os.listdir(long_term_ftr_results_path):\n",
    "    folder_path = os.path.join(long_term_ftr_results_path, folder_name)\n",
    "    if ('2025-2028' in folder_path) or ('2026-2029' in folder_path): # to be set by analyst - can be automated\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            if 'round' in file_path:\n",
    "                print(f'Aggregating long-term FTR results from {file_name}')\n",
    "                df_long_term_ftr_results_temp = pd.read_excel(file_path, sheet_name='Obligation Nodal Prices RD ' + file_path[-6]) # file_path[-6] gets the round number from the file name\n",
    "                df_long_term_ftr_results_temp['LT_Round'] = file_path[-6]\n",
    "                df_long_term_ftr_results_temp['LT_Period'] = folder_name\n",
    "                df_long_term_ftr_results = pd.concat([df_long_term_ftr_results, df_long_term_ftr_results_temp])             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c5cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_term_ftr_results = df_long_term_ftr_results.iloc[:, [0, 1, 2, 3, 8, 9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3aeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking where NaNs exist and drop them\n",
    "\n",
    "df_long_term_ftr_results[df_long_term_ftr_results.isna().any(axis=1)]\n",
    "\n",
    "df_long_term_ftr_results.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8b291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_term_ftr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bacf642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganizing data\n",
    "\n",
    "df_long_term_ftr_results = df_long_term_ftr_results.pivot_table(\n",
    "    index=['PNODEID', 'Node'],\n",
    "    columns=['LT_Round', 'Period Type', 'LT_Period'], # this is the convention for naming the column\n",
    "    values='LMP'\n",
    ")\n",
    "\n",
    "df_long_term_ftr_results.columns = ['_'.join(col) for col in df_long_term_ftr_results.columns.to_flat_index()] # flattening MultiIndex columns\n",
    "\n",
    "df_long_term_ftr_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd78071",
   "metadata": {},
   "source": [
    "## ARR Valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5a9615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering by zone selected by analyst, filtering out rows with 0 capacity, filtering out rows with retired and rate-based resources.\n",
    "\n",
    "df_stage_1_resources_filtered = df_stage_1_resources[(df_stage_1_resources['Zone'] == zone) & (df_stage_1_resources[current_planning_year + '_Capacity MW'] > 0) \n",
    "                                                     & (df_stage_1_resources['Retired'] != 'Y') & (df_stage_1_resources['Rate_based'] != 'Rate-based')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd3946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stage_1_resources_filtered = df_stage_1_resources_filtered.drop(\n",
    "    columns=['Retired', 'Rate_based'] \n",
    ").groupby( # There are some paths with duplicate PNODEIDs and FTR names with different capacities, so we group\n",
    "   ['Zone', 'PNODEID', 'FTR Name'] \n",
    ").sum().reset_index()\n",
    "\n",
    "df_stage_1_resources_filtered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c2d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging with annual FTR auction results\n",
    "\n",
    "df_arr_valuation = df_stage_1_resources_filtered.merge(\n",
    "    right=df_annual_ftr_results,\n",
    "    how='left',\n",
    "    on='PNODEID',\n",
    "    validate='m:1'\n",
    ")\n",
    "\n",
    "df_arr_valuation.drop(columns=['1', '2', '3', '4', 'FTR Name'], inplace=True) # Dropping columns 1, 2, 3, 4 since we already calculated the average and dropping FTR name since it \n",
    "# should be the same as the node\n",
    "\n",
    "df_arr_valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e7c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging with long-term FTR auction results\n",
    "\n",
    "df_arr_valuation = df_arr_valuation.merge(\n",
    "    right=df_long_term_ftr_results,\n",
    "    how='left',\n",
    "    on='PNODEID',\n",
    "    validate='m:1',\n",
    ")\n",
    "\n",
    "df_arr_valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabca488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating path value of LT auctions: Sink congestion - source congestion\n",
    "\n",
    "df_arr_valuation.iloc[:, 5:] = df_long_term_ftr_results[df_long_term_ftr_results.index.get_level_values(0) == sink_id].values - df_arr_valuation.iloc[:, 5:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff042ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating annual path value: Sink congestion - source congestion\n",
    "\n",
    "df_arr_valuation[current_planning_year + '_path_value'] = df_annual_ftr_results[df_annual_ftr_results.PNODEID == sink_id]['Average_LMP'].values - df_arr_valuation['Average_LMP'] # Path value = sink LMP - source LMP\n",
    "\n",
    "# dropping\n",
    "df_arr_valuation.drop(\n",
    "    columns='Average_LMP',\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748585e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out paths whose values are below the threshold\n",
    "\n",
    "temp_year = current_planning_year # just for the loop below\n",
    "\n",
    "for i in range(look_ahead_years):\n",
    "    if i == 0: # for current planning year\n",
    "        df_arr_valuation[temp_year + '_Selection'] = df_arr_valuation.apply(\n",
    "            lambda row: 1 if row[temp_year + '_path_value'] > selection_threshold else 0, axis=1 \n",
    "        )\n",
    "    else:\n",
    "        df_arr_valuation[temp_year + '_Selection'] = df_arr_valuation.apply(\n",
    "            lambda row: 1 if row[most_recent_LT_auction_round + '_' + 'YR' + str(i) + '_' + most_recent_LT_auction] > selection_threshold else 0, axis=1\n",
    "        )\n",
    "    temp_year = next_planning_year(temp_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1316e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arr_valuation # This should have the all the annual and LT path values along with the respective selections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1697aab3",
   "metadata": {},
   "source": [
    "Manually setting specific path selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703213a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEPCO - DC\n",
    "\n",
    "# df_arr_valuation.loc[\n",
    "#     lambda DF: DF.Node.isin(\n",
    "#         [\n",
    "#         'SMECO   13 KV   SMECO', 'CHALKPT 24 KV   CHLKG4',\n",
    "#        'CHALKPT 24 KV   CHLKG3', 'CHALKPT 13 KV   CT6',\n",
    "#        'CHALKPT 13 KV   CT4', 'CHALKPT 13 KV   CT3',\n",
    "#        'CHALKPT 13 KV   CT5', 'CHALKPT 13 KV   CT2',\n",
    "#        'CHALKPT 4 KV    CT1', 'MORGANTO13 KV   CT3',\n",
    "#        'MORGANTO13 KV   CT4', 'MORGANTO13 KV   CT5',\n",
    "#        'MORGANTO13 KV   CT6', 'KELSONRI18 KV   STCHA1CT'\n",
    "#         ]\n",
    "#     ),\n",
    "# [next_year + '_Selection', next_to_next_year + '_Selection', next_to_next_to_next_year + '_Selection']\n",
    "# ] = 0\n",
    "\n",
    "# APS\n",
    "\n",
    "# df_arr_valuation.loc[\n",
    "#     (df_arr_valuation.Node == 'GREENGAP35 KV   G1'), [next_to_next_year + '_Selection', next_to_next_to_next_year + '_Selection']\n",
    "# ] = 0\n",
    "\n",
    "# PENELEC\n",
    "\n",
    "# df_arr_valuation.loc[\n",
    "#     (df_arr_valuation.Node == 'WARREN  13 KV   UNITCT'), [next_to_next_year + '_Selection', next_to_next_to_next_year + '_Selection']\n",
    "# ] = 0\n",
    "\n",
    "# df_arr_valuation.to_clipboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612402b",
   "metadata": {},
   "source": [
    "## Long-term decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d9b975",
   "metadata": {},
   "source": [
    "### Historical Data Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b35ba4",
   "metadata": {},
   "source": [
    "Aggregating annual FTR auction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9635b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_annual_ftr_results = pd.DataFrame()\n",
    "\n",
    "for folder_name in os.listdir(annual_ftr_results_path):\n",
    "    \n",
    "    folder_path = os.path.join(annual_ftr_results_path, folder_name)\n",
    "\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            if 'round' in file_path:\n",
    "                print(f'Aggregating annual FTR results from {file_name}')\n",
    "                if file_path[-14].isnumeric():\n",
    "                    round_num = file_path[-14] # for xlsx files\n",
    "                else:\n",
    "                    round_num = file_path[-13] # for xls files\n",
    "                df_all_annual_ftr_results_temp = pd.read_excel(file_path, sheet_name='Obligation Nodal Prices RD ' + round_num)\n",
    "                df_all_annual_ftr_results_temp['Round'] = round_num\n",
    "                df_all_annual_ftr_results_temp['Year'] = folder_name\n",
    "                df_all_annual_ftr_results = pd.concat([df_all_annual_ftr_results, df_all_annual_ftr_results_temp])\n",
    "\n",
    "df_all_annual_ftr_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b27bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_annual_ftr_results_pivoted = df_all_annual_ftr_results.iloc[:, [0, 1, 3, 6, 7]].dropna().pivot_table(\n",
    "    index='PNODEID',\n",
    "    columns='Year',\n",
    "    values='LMP'\n",
    ") # this should average across all rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcf2baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_annual_ftr_results_pivoted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03700f3b",
   "metadata": {},
   "source": [
    "Aggregating long-term FTR auction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3308762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_long_term_ftr_results = pd.DataFrame()\n",
    "\n",
    "for folder_name in os.listdir(long_term_ftr_results_path):\n",
    "    folder_path = os.path.join(long_term_ftr_results_path, folder_name)\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if 'round' in file_path:\n",
    "            if file_path[-6].isnumeric():\n",
    "                round_num = file_path[-6] # for xlsx files\n",
    "            else:\n",
    "                round_num = file_path[-5] # for xls files\n",
    "            print(f'Aggregating long-term FTR results from {file_name}')\n",
    "            df_all_long_term_ftr_results_temp = pd.read_excel(file_path, sheet_name='Obligation Nodal Prices RD ' + round_num) \n",
    "            df_all_long_term_ftr_results_temp['LT_Round'] = round_num\n",
    "            df_all_long_term_ftr_results_temp['LT_Period'] = folder_name\n",
    "            df_all_long_term_ftr_results = pd.concat([df_all_long_term_ftr_results, df_all_long_term_ftr_results_temp])\n",
    "\n",
    "df_all_long_term_ftr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8ed07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_long_term_ftr_results = df_all_long_term_ftr_results[df_all_long_term_ftr_results['Period Type'] != 'YRALL'] # dropping rows where period type is YRALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ee2de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_long_term_ftr_results = df_all_long_term_ftr_results.iloc[:, [0, 1, 2, 3, 6, 7]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb53ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_long_term_ftr_results['Year'] = df_all_long_term_ftr_results.apply(planning_year_from_LT_auction, axis=1) # adding the planning year based on the LT period and the period type\n",
    "\n",
    "df_all_long_term_ftr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9319667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_long_term_ftr_results_pivoted = df_all_long_term_ftr_results.pivot_table(\n",
    "    index='PNODEID',\n",
    "    columns=['Year', 'Period Type', 'LT_Round'], \n",
    "    values='LMP'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_long_term_ftr_results_pivoted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dccb880",
   "metadata": {},
   "source": [
    "### Long-term decay calculation for given sink and selected paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6184badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the annual path value for the given sink: sink congestion - source congestion\n",
    "\n",
    "df_all_annual_ftr_results_path_values = df_all_annual_ftr_results_pivoted[df_all_annual_ftr_results_pivoted.index == sink_id].values - df_all_annual_ftr_results_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e24270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the long-term path value for the given sink: sink congestion - source congestion\n",
    "\n",
    "df_all_long_term_ftr_results_path_values = df_all_long_term_ftr_results_pivoted.loc[df_all_long_term_ftr_results_pivoted.index == sink_id].values -  df_all_long_term_ftr_results_pivoted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd556b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long-term decay calculation\n",
    "\n",
    "df_long_term_decay = df_all_annual_ftr_results_path_values.div(df_all_long_term_ftr_results_path_values.replace(0, np.nan)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84270f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding outliers - 2022-2023 had outliers due to Russia-Ukraine\n",
    "\n",
    "# df_long_term_decay.drop(\n",
    "#     '2022-2023', axis=1, level=0,\n",
    "#     inplace=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738112c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_term_decay = df_long_term_decay.groupby(\n",
    "    axis=1,\n",
    "    level=[1, 2],\n",
    ").mean() # Averaging across the different years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84603496",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_term_decay.columns = df_long_term_decay.columns.to_flat_index() # Flattening MultiIndex to merge in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28eb770",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_term_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e838111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_long_term_decay.loc[\n",
    "#     lambda DF: DF.index.isin([1097732449, 50558, 50557, 50489, 50490]) \n",
    "# ].to_excel('PSEG_2025-2026_4_LTD_5_largest_paths_all_history.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a45870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "# df_arr_valuation[['PNODEID', current_planning_year + '_Capacity MW', next_year + '_Selection']].loc[\n",
    "#     lambda DF: DF['2026-2027_Selection'] == 1\n",
    "# ].sort_values(\n",
    "#     by=['2025-2026_Capacity MW'],\n",
    "#     ascending=False\n",
    "# )\n",
    "# .iloc[[0, 3, 4, 5, 6], :].PNODEID.values\n",
    "# .iloc[[i for i in range(24) if i not in [0, 3, 4, 5, 6]], :]\n",
    "# )\n",
    "# df_arr_valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9de3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging df_long_term_decay with df_arr_valuation to get the decay for the relevant paths\n",
    "\n",
    "df_long_term_decay_filtered = df_arr_valuation[['PNODEID', current_planning_year + '_Capacity MW', next_year + '_Selection', most_recent_LT_auction_round + '_' + 'YR1' + '_' + most_recent_LT_auction]].merge( # Note that we use next year's selection\n",
    "    right=df_long_term_decay,\n",
    "    on='PNODEID',\n",
    "    how='left',\n",
    "    validate='m:1'\n",
    ")\n",
    "\n",
    "\n",
    "# Sometimes they want to see what the LTD would be if we don't filter any paths by the threshold. In that case, just replace the df_long_term_decay_filtered[current_planning_year + '_Selection']\n",
    "# below by 1\n",
    "\n",
    "# Usually, we just multiply by capacity, but as a different modeling appraoch, I also multiplied by the path value so that we can dollar weight it instead\n",
    "\n",
    "# I call it selected capacity on the next line but it is actually selected value\n",
    "df_long_term_decay_filtered[next_year + '_Selected_Capacity'] = df_long_term_decay_filtered[current_planning_year + '_Capacity MW'] * df_long_term_decay_filtered[next_year + '_Selection'] * df_long_term_decay_filtered[most_recent_LT_auction_round + '_' + 'YR1' + '_' + most_recent_LT_auction].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d003941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping these columns since we have already multiplied the selection (1 or 0) by the path MW in the previous step\n",
    "\n",
    "df_long_term_decay_filtered.drop(\n",
    "    columns=['PNODEID', current_planning_year + '_Capacity MW', next_year + '_Selection', most_recent_LT_auction_round + '_' + 'YR1' + '_' + most_recent_LT_auction], inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdfabde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all_annual_ftr_results_pivoted\n",
    "\n",
    "df_long_term_decay_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaec7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_long_term_decay_filtered[('YR1', '4')].to_clipboard(index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26dec4",
   "metadata": {},
   "source": [
    "Calculating the MW (treated as frequency) - weighted quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ca1848",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = [(i + 1) / 10 for i in range(-1, 10)] # Percentiles from 0% to 100%\n",
    "num_columns_ltd = len(df_long_term_decay_filtered.drop(\n",
    "    columns=next_year + '_Selected_Capacity'\n",
    ").columns) # All the 'YR_', '' columns in df_long_term_decay_filtered\n",
    "\n",
    "np_ltd = np.zeros((len(percentiles), num_columns_ltd)) # Creating numpy array to store results\n",
    "\n",
    "for percentile in range(len(percentiles)):\n",
    "    for column in range(num_columns_ltd):\n",
    "        np_ltd[percentile, column] = wq.quantile(\n",
    "            np.array(df_long_term_decay_filtered.drop(\n",
    "            columns=next_year + '_Selected_Capacity'\n",
    "        ))[:, column],\n",
    "        df_long_term_decay_filtered[next_year + '_Selected_Capacity'], percentiles[percentile]\n",
    "        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b0a10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_term_decay_summary_stats = pd.DataFrame(np_ltd)\n",
    "\n",
    "df_long_term_decay_summary_stats.index = percentiles # Naming the index\n",
    "\n",
    "df_long_term_decay_summary_stats.columns = df_long_term_decay_filtered.drop(\n",
    "    columns=next_year + '_Selected_Capacity'\n",
    ").columns # Naming the columns\n",
    "\n",
    "df_long_term_decay_summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce84b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a P table for LTD - original approach\n",
    "\n",
    "# df_long_term_decay_summary_stats = df_long_term_decay_filtered.drop(columns=current_planning_year + '_Selected_Capacity').describe(\n",
    "#     percentiles=[(i + 1) / 10 for i in range(9)],\n",
    "# )\n",
    "\n",
    "# df_long_term_decay_summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f44b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the capacity weighted average (that has now been updated to the value weighted average)\n",
    "\n",
    "capacity_weights = df_long_term_decay_filtered[next_year + '_Selected_Capacity'].values / sum(df_long_term_decay_filtered[next_year + '_Selected_Capacity'].values)\n",
    "\n",
    "capacity_weighted_average = df_long_term_decay_filtered.drop(columns=next_year + '_Selected_Capacity').mul(capacity_weights, axis=0).sum(axis=0).values\n",
    "\n",
    "df_long_term_decay_summary_stats.loc['Value Weighted Average'] =  capacity_weighted_average.tolist()\n",
    "\n",
    "df_long_term_decay_summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150caef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the standard outputs with the capacity weighted outputs and outputting to Excel - this is now also combined with the valuation tables\n",
    "\n",
    "pd.concat([df_long_term_decay_filtered.drop(columns=next_year + '_Selected_Capacity').describe(), df_long_term_decay_summary_stats]).to_excel(\n",
    "    f'{zone}_{current_planning_year}_{look_ahead_years}_LTD.xlsx'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece3e91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original approach\n",
    "\n",
    "# df_long_term_decay_summary_stats.to_excel(f'{zone}_{current_planning_year}_{look_ahead_years}_LTD.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e6fa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wq.quantile(\n",
    "#     np.array([-0.5, 0.05, -0.2, 0.15]),\n",
    "#     np.array([5, 10, 15, 20]),\n",
    "#     0.5\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e288c3",
   "metadata": {},
   "source": [
    "## Stage 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463dc4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_stage_1B = pd.concat([pd.read_excel(stage_1B_path, sheet_name='ARR'), pd.read_excel(stage_1B_path, sheet_name='IARR')]) # Told not to include IARR\n",
    "\n",
    "df_stage_1B = pd.read_excel(stage_1B_path, sheet_name='ARR')\n",
    "\n",
    "df_stage_1B = df_stage_1B.iloc[:, :6] # dropping columns not needed\n",
    "\n",
    "df_stage_1B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b49810b",
   "metadata": {},
   "source": [
    "Merging with nodal congestions from annual and long-term auctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85ae550",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stage_1B = df_stage_1B.merge(\n",
    "    right=df_annual_ftr_results,\n",
    "    left_on='ns1:SinkName',\n",
    "    right_on='Node'\n",
    ").drop(columns=['1', '2', '3', '4', 'Node', 'PNODEID']).rename(columns={'Average_LMP': 'Annual Sink LMP'}).merge(\n",
    "    right=df_annual_ftr_results,\n",
    "    left_on='ns1:SourceName',\n",
    "    right_on='Node'\n",
    ").drop(columns=['1', '2', '3', '4', 'Node', 'PNODEID']).rename(columns={'Average_LMP': 'Annual Source LMP'}).merge(\n",
    "    right=df_long_term_ftr_results,\n",
    "    left_on='ns1:SinkName', # x - Sink\n",
    "    right_on='Node'\n",
    ").merge(\n",
    "    right=df_long_term_ftr_results,\n",
    "    left_on='ns1:SourceName', # y - Source\n",
    "    right_on='Node'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c20f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_year = current_planning_year # just for the loop below\n",
    "\n",
    "for i in range(look_ahead_years):\n",
    "    if i == 0: # for current planning year\n",
    "        df_stage_1B[temp_year + '_path_value'] = df_stage_1B['Annual Sink LMP'] - df_stage_1B['Annual Source LMP']\n",
    "    else:\n",
    "        df_stage_1B[temp_year + '_path_value'] = df_stage_1B[most_recent_LT_auction_round + '_' + 'YR' + str(i) + '_' + most_recent_LT_auction + '_x'] - df_stage_1B[most_recent_LT_auction_round + '_' + 'YR' + str(i) + '_' + most_recent_LT_auction + '_y']\n",
    "    temp_year = next_planning_year(temp_year)\n",
    "\n",
    "df_stage_1B = df_stage_1B[df_stage_1B['ns1:SinkName'] == sink_name] # filtering by sink\n",
    "\n",
    "df_stage_1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557b1116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually filtering out paths with very low path values - this is not standard for every deal\n",
    "\n",
    "# df_stage_1B.loc[(df_stage_1B[current_planning_year + '_path_value'] <= 20) & (~df_stage_1B['ns1:ParticipantName'].isin(['FUNEPL', 'FPLG40'])), 'ns1:ClearedMW'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a55fd7",
   "metadata": {},
   "source": [
    "## Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5456e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stage_2 = pd.concat([pd.read_excel(stage_2_path_r1, sheet_name='PATHS').iloc[:, :6], pd.read_excel(stage_2_path_r2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7516b0b1",
   "metadata": {},
   "source": [
    "Merging with source and sink congestions from annual auction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f95034",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stage_2 = df_stage_2.merge(\n",
    "    right=df_annual_ftr_results,\n",
    "    left_on='ns1:SinkName',\n",
    "    right_on='Node',\n",
    "    validate='m:1'\n",
    ").drop(\n",
    "    columns=['Node','PNODEID', '1', '2', '3', '4']\n",
    ").rename(\n",
    "    columns={\n",
    "        'Average_LMP': 'Sink_LMP'\n",
    "    }\n",
    ").merge(\n",
    "    right=df_annual_ftr_results,\n",
    "    left_on='ns1:SourceName',\n",
    "    right_on='Node',\n",
    "    validate='m:1'    \n",
    ").drop(\n",
    "    columns=['Node','PNODEID', '1', '2', '3', '4']\n",
    ").rename(\n",
    "    columns={\n",
    "        'Average_LMP': 'Source_LMP'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8b648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stage_2['Path_Value'] = df_stage_2['Sink_LMP'] - df_stage_2['Source_LMP']\n",
    "df_stage_2['Total_Value'] = df_stage_2['Path_Value'] * df_stage_2['ns1:ClearedMW']\n",
    "\n",
    "df_stage_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b959876",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subaccounts_sinks = pd.read_excel(stage_1A_pull_path, sheet_name='NSPL')\n",
    "\n",
    "subaccounts = df_subaccounts_sinks[df_subaccounts_sinks['sinkName'] == sink_name]['participantName'].unique()\n",
    "\n",
    "df_stage_2_filtered = df_stage_2[df_stage_2['ns1:ParticipantName'].isin(subaccounts)] # filtering by sink\n",
    "\n",
    "df_stage_2_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae60a3f4",
   "metadata": {},
   "source": [
    "The below analysis was only done for UGI which had no stage 1A or 1B paths - this is not a standard thing that had to be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e5fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_stage_2_filtered.merge(\n",
    "#     right=df_long_term_ftr_results[[most_recent_LT_auction_round + '_' + 'YR1' + '_' + most_recent_LT_auction]].reset_index(),\n",
    "#     left_on='ns1:SinkName',\n",
    "#     right_on='Node'\n",
    "# ).rename(\n",
    "#     columns={\n",
    "#         most_recent_LT_auction_round + '_' + 'YR1' + '_' + most_recent_LT_auction: 'Sink_LT_LMP'\n",
    "#     }\n",
    "# ).merge(\n",
    "#     right=df_long_term_ftr_results[[most_recent_LT_auction_round + '_' + 'YR1' + '_' + most_recent_LT_auction]].reset_index(),\n",
    "#     left_on='ns1:SourceName',\n",
    "#     right_on='Node'\n",
    "# ).rename(\n",
    "#     columns={\n",
    "#         most_recent_LT_auction_round + '_' + 'YR1' + '_' + most_recent_LT_auction: 'Source_LT_LMP'\n",
    "#     }\n",
    "# ).drop(\n",
    "#     columns=['PNODEID_x', 'Node_x', 'PNODEID_y', 'Node_y']\n",
    "# ).assign(\n",
    "#     LT_path_value=lambda DF: DF.Sink_LT_LMP - DF.Source_LT_LMP\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b02045",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stage_2_filtered['Total_Value'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d67578",
   "metadata": {},
   "source": [
    "## Congestion settles for Stage 1A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3c068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_arr_valuation_pseg_congestion = df_arr_valuation.loc[\n",
    "#     lambda DF: DF[next_year + '_Selection'] == 1\n",
    "# ].sort_values(\n",
    "#     by='2025-2026_Capacity MW',\n",
    "#     ascending=False\n",
    "# ).iloc[:7, :]\n",
    "\n",
    "# df_arr_valuation_pseg_congestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d036d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stage_1A_congestion_settles = pd.DataFrame() # initializing empty DF\n",
    "\n",
    "# Pulling congestion LMPs for the source nodes (and sink) of the paths selected in stage 1A\n",
    "\n",
    "# Appending sink ID to selected sources\n",
    "\n",
    "# Normally, we do it for all the selections. For PSEG, we will just look at specific paths. To revert, replace df_arr_valuation_pseg_congestion with df_arr_valuation or vice versa\n",
    "\n",
    "selected_nodes = np.append(df_arr_valuation.loc[df_arr_valuation[next_year + '_Selection'] == 1].PNODEID.unique().astype(int), sink_id)\n",
    "\n",
    "selected_capacity = df_arr_valuation.loc[df_arr_valuation[next_year + '_Selection'] == 1][current_planning_year + '_Capacity MW'].values\n",
    "\n",
    "for pnode_id in selected_nodes:\n",
    "    df_stage_1A_congestion_settles_temp = pull_lmp_data(\n",
    "    emtdb=emtdb,\n",
    "    pnode_id=pnode_id,\n",
    "    da_or_rt='DA',\n",
    "    start_dt='2019-06-01',\n",
    "    end_dt=pd.Timestamp.today().date() - pd.offsets.MonthEnd(),\n",
    "    price_data_type='CONGESTION'\n",
    ").reset_index().assign(\n",
    "    PNODEID=pnode_id\n",
    ")\n",
    "    \n",
    "    df_stage_1A_congestion_settles = pd.concat([df_stage_1A_congestion_settles, df_stage_1A_congestion_settles_temp], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f53c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stage_1A_congestion_settles = df_stage_1A_congestion_settles.assign(\n",
    "    Date=lambda DF: DF.Date.apply(pd.to_datetime)\n",
    ")\n",
    "\n",
    "df_stage_1A_congestion_settles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cf79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stage_1A_congestion_settles_pivoted = df_stage_1A_congestion_settles.pivot(\n",
    "    index=['Date', 'Hour'],\n",
    "    columns='PNODEID',\n",
    "    values='Price'\n",
    ").pipe(\n",
    "    lambda DF: DF.sub(DF[sink_id], axis=0) # Here we're doing source - sink and then multiplying by the negative of the capacity in the next step\n",
    ").drop(\n",
    "    columns=sink_id\n",
    ").reindex(columns=selected_nodes[:-1]).multiply(-selected_capacity).reset_index().assign( \n",
    "    Planning_year=lambda DF: DF.apply(planning_year_from_date, axis=1) # Helper column planning year\n",
    ").assign(\n",
    "    Month=lambda DF: DF.Date.dt.month # Helper column month\n",
    ").assign(\n",
    "    All_Paths=lambda DF: DF[selected_nodes[:-1]].sum(axis=1)\n",
    ").drop(columns=selected_nodes[:-1]).pivot_table(\n",
    "    index='Month',\n",
    "    columns='Planning_year',\n",
    "    values='All_Paths',\n",
    "    aggfunc='sum'\n",
    ").reindex([6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5])\n",
    "\n",
    "df_stage_1A_congestion_settles_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6dd9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=df_stage_1A_congestion_settles_pivoted,\n",
    "        x=df_stage_1A_congestion_settles_pivoted.columns,\n",
    "        y=df_stage_1A_congestion_settles_pivoted.index.astype(str),\n",
    "        colorscale='Jet'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d7d861",
   "metadata": {},
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d491432",
   "metadata": {},
   "source": [
    "### Reading zonal NSPL and NEE NSPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36066fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "zonal_NSPL_for_ARR = pd.read_excel(zonal_nspl_path, sheet_name='Stage 1A MW - Update HH 12-10').pipe( # This tab was created by me by copying the existing Stage 1A MW tab and updating it for PJM's latest values \n",
    "    lambda DF: DF[DF.ZONE==zone]\n",
    ").iloc[:, 2].values # extracting zonal NSPL corresponding to zone\n",
    "\n",
    "\n",
    "NEE_NSPL_for_ARR = pd.read_excel(nee_nspl_path, sheet_name='NSPL').pipe(\n",
    "    lambda DF: DF[DF.sinkName == sink_name]\n",
    ")['ns1:NetworkServicePeakLoad'].sum() # summing up NEE NSPL corresponding to sink\n",
    "\n",
    "pmi_share = NEE_NSPL_for_ARR / zonal_NSPL_for_ARR\n",
    "\n",
    "clear_rate = 1 # Set this to 1 by default\n",
    "\n",
    "stage_2_dollars_per_MW_PY_2026_2027_base = pd.read_excel(stage_2_dollar_per_MW_PY_26_27_path, usecols='M', skiprows=7, sheet_name='PY 2627 Summary', nrows=1).iloc[0, 0] * clear_rate # Cell M9\n",
    "stage_2_dollars_per_MW_PY_2027_2028_base = pd.read_excel(stage_2_dollar_per_MW_PY_27_28_path, usecols='M', skiprows=7, sheet_name='PY 2728 Summary', nrows=1).iloc[0, 0] * clear_rate # Cell M9\n",
    "\n",
    "stage_2_dollars_per_MW_PY_2026_2027_good = pd.read_excel(stage_2_dollar_per_MW_PY_26_27_path, usecols='N', skiprows=7, sheet_name='PY 2627 Summary', nrows=1).iloc[0, 0] * clear_rate # Cell N9\n",
    "stage_2_dollars_per_MW_PY_2027_2028_good = pd.read_excel(stage_2_dollar_per_MW_PY_27_28_path, usecols='N', skiprows=7, sheet_name='PY 2728 Summary', nrows=1).iloc[0, 0] * clear_rate # Cell N9\n",
    "\n",
    "stage_2_dollars_per_MW_PY_2026_2027_bad = stage_2_dollars_per_MW_PY_2026_2027_base * 25 / 28\n",
    "stage_2_dollars_per_MW_PY_2027_2028_bad = stage_2_dollars_per_MW_PY_2027_2028_base * 25 / 28\n",
    "\n",
    "\n",
    "stage_1B_MW = df_stage_1B['ns1:ClearedMW'].sum() / pmi_share # This is what it is normally but if stage 1B is negative, we set it to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45161d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "zonal_NSPL_for_ARR, NEE_NSPL_for_ARR, pmi_share, stage_1B_MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90073f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_MW_for_ARR = percent_for_ARR * zonal_NSPL_for_ARR\n",
    "Total_MW_for_stage_1A = percent_for_stage_1A * Total_MW_for_ARR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1753432",
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_MW_for_ARR, Total_MW_for_stage_1A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a507352a",
   "metadata": {},
   "source": [
    "### Helper function for doing the base, good and bad cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7f91f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arr_valuation_output(annual_decay: list, stage_2_dollars_per_MW: list):\n",
    "    planning_years = []\n",
    "    raw_stage_1_dollars = []\n",
    "    stage_1_dollars = []\n",
    "    annual_path_MWs = []\n",
    "    adj_ratios = []\n",
    "    MW_for_stage_2 = []\n",
    "    stage_1B_dollars = []\n",
    "    stage_2_dollars = []\n",
    "    total_dollars = []\n",
    "\n",
    "    temp_year = current_planning_year # just for the loop below\n",
    "\n",
    "    for i in range(look_ahead_years):\n",
    "\n",
    "        planning_years.append(temp_year)\n",
    "        \n",
    "        annual_path_MWs.append(round((df_arr_valuation[current_planning_year + '_Capacity MW'] * df_arr_valuation[temp_year + '_Selection']).sum()))\n",
    "\n",
    "        if temp_year == current_planning_year:\n",
    "            # print(i)\n",
    "            adj_ratios.append(1)\n",
    "            \n",
    "            raw_stage_1_dollars.append((df_arr_valuation[current_planning_year + '_Capacity MW'] * df_arr_valuation[temp_year + '_path_value'] * \n",
    "                                    df_arr_valuation[temp_year + '_Selection']).sum())        \n",
    "            \n",
    "            stage_1_dollars.append((df_arr_valuation[current_planning_year + '_Capacity MW'] * df_arr_valuation[temp_year + '_path_value'] * \n",
    "                                    df_arr_valuation[temp_year + '_Selection']).sum() * adj_ratios[i] * (1 + annual_decay[i]) ** i )\n",
    "            \n",
    "            stage_1B_dollars.append((df_stage_1B[temp_year + '_path_value'] * df_stage_1B['ns1:ClearedMW']).sum() / pmi_share / 2)\n",
    "            \n",
    "            stage_2_dollars.append(df_stage_2_filtered['Total_Value'].sum() / pmi_share / 2)\n",
    "            \n",
    "            MW_for_stage_2.append(0) # we don't need this for the current planning year since we get the pool of dollars\n",
    "        \n",
    "        else: \n",
    "            adj_ratios.append(min(1, Total_MW_for_stage_1A / annual_path_MWs[i])) # This was i - 1 earlier\n",
    "            # adj_ratios.append(1) # Use this for zones with no stage 1A MW\n",
    "            raw_stage_1_dollars.append((df_arr_valuation[current_planning_year + '_Capacity MW'] * df_arr_valuation[most_recent_LT_auction_round + '_YR' + str(i) + '_' + most_recent_LT_auction] * \n",
    "                                    df_arr_valuation[temp_year + '_Selection']).sum())        \n",
    "            stage_1_dollars.append((df_arr_valuation[current_planning_year + '_Capacity MW'] * df_arr_valuation[most_recent_LT_auction_round + '_YR' + str(i) + '_' + most_recent_LT_auction] * \n",
    "                                    df_arr_valuation[temp_year + '_Selection']).sum() * adj_ratios[i] * (1 + annual_decay[i]) ** i)\n",
    "            \n",
    "            stage_1B_dollars.append((df_stage_1B[temp_year + '_path_value'] * df_stage_1B['ns1:ClearedMW']).sum() / pmi_share)\n",
    "            \n",
    "            MW_for_stage_2.append(Total_MW_for_ARR - annual_path_MWs[i] * adj_ratios[i] - stage_1B_MW)\n",
    "            \n",
    "            stage_2_dollars.append(MW_for_stage_2[i] * stage_2_dollars_per_MW[i])\n",
    "        \n",
    "        total_dollars.append(stage_1_dollars[i] + stage_1B_dollars[i] + stage_2_dollars[i])\n",
    "        temp_year = next_planning_year(temp_year)\n",
    "\n",
    "    return pd.DataFrame([annual_path_MWs,\n",
    "          raw_stage_1_dollars,\n",
    "          adj_ratios,\n",
    "          annual_decay,\n",
    "          stage_1_dollars,\n",
    "          stage_1B_dollars,\n",
    "          MW_for_stage_2,\n",
    "          stage_2_dollars_per_MW,\n",
    "          stage_2_dollars,\n",
    "          total_dollars,\n",
    "          ], columns=planning_years, index=[\n",
    "            'annual_path_MWs',\n",
    "            'raw_stage_1_dollars', \n",
    "          'adj_ratios',\n",
    "          'annual_decay',\n",
    "          'stage_1_dollars',\n",
    "          'stage_1B_dollars',\n",
    "          'MW_for_stage_2',\n",
    "          'stage_2_dollars_per_MW',\n",
    "          'stage_2_dollars',\n",
    "          'total_dollars',   \n",
    "          ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b048af5f",
   "metadata": {},
   "source": [
    "### Analyst to set good, base and bad LTD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18421df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AECO\n",
    "# good_decay = -0.22 \n",
    "# base_decay = -0.27\n",
    "# bad_decay = -0.31\n",
    "\n",
    "# JCPL\n",
    "# good_decay = -0.11 \n",
    "# base_decay = -0.14\n",
    "# bad_decay = -0.18\n",
    "\n",
    "# PSEG\n",
    "# good_decay = -0.06 \n",
    "# base_decay = -0.14\n",
    "# bad_decay = -0.24\n",
    "\n",
    "# RECO\n",
    "# good_decay = 0\n",
    "# base_decay = 0\n",
    "# bad_decay = 0\n",
    "\n",
    "# UGI\n",
    "# good_decay = -0.15\n",
    "# base_decay = -0.11\n",
    "# bad_decay = -0.13\n",
    "\n",
    "# DPL\n",
    "good_decay = -0.16\n",
    "base_decay = -0.15\n",
    "bad_decay = -0.13\n",
    "\n",
    "annual_decay_good = []\n",
    "annual_decay_base = []\n",
    "annual_decay_bad = []\n",
    "\n",
    "for i in range(look_ahead_years):\n",
    "    if i == 0:\n",
    "        annual_decay_good.append(0)\n",
    "        annual_decay_base.append(0)\n",
    "        annual_decay_bad.append(0)\n",
    "    else:\n",
    "        annual_decay_good.append(good_decay)\n",
    "        annual_decay_base.append(base_decay)\n",
    "        annual_decay_bad.append(bad_decay)\n",
    "\n",
    "stage_2_dollars_per_MW_good = [0, stage_2_dollars_per_MW_PY_2026_2027_good, stage_2_dollars_per_MW_PY_2027_2028_good, stage_2_dollars_per_MW_PY_2027_2028_good]\n",
    "stage_2_dollars_per_MW_base = [0, stage_2_dollars_per_MW_PY_2026_2027_base, stage_2_dollars_per_MW_PY_2027_2028_base, stage_2_dollars_per_MW_PY_2027_2028_base]\n",
    "stage_2_dollars_per_MW_bad = [0, stage_2_dollars_per_MW_PY_2026_2027_bad, stage_2_dollars_per_MW_PY_2027_2028_bad, stage_2_dollars_per_MW_PY_2027_2028_bad]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d21f47",
   "metadata": {},
   "source": [
    "### ARR Valuation Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238de07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(f'{zone}_{current_planning_year}_{look_ahead_years}_valuation.xlsx', engine='openpyxl') as writer:\n",
    "    df_arr_valuation.to_excel(writer, sheet_name='Paths')\n",
    "    arr_valuation_output(annual_decay_base, stage_2_dollars_per_MW_base).to_excel(writer, sheet_name='Base')\n",
    "    arr_valuation_output(annual_decay_good, stage_2_dollars_per_MW_good).to_excel(writer, sheet_name='Good')\n",
    "    arr_valuation_output(annual_decay_bad, stage_2_dollars_per_MW_bad).to_excel(writer, sheet_name='Bad')\n",
    "    pd.concat([df_long_term_decay_filtered.drop(columns=next_year + '_Selected_Capacity').describe(), df_long_term_decay_summary_stats]).to_excel(writer, sheet_name='LTD')\n",
    "    df_stage_1A_congestion_settles_pivoted.to_excel(writer, sheet_name='Congestion Settles')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb05169",
   "metadata": {},
   "source": [
    "Below cell just for manually outputting to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fba388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr_valuation_output(annual_decay_base, stage_2_dollars_per_MW_base).to_clipboard()\n",
    "# arr_valuation_output(annual_decay_good, stage_2_dollars_per_MW_good).to_clipboard()\n",
    "# arr_valuation_output(annual_decay_bad, stage_2_dollars_per_MW_bad).to_clipboard()\n",
    "# pd.concat([df_long_term_decay_filtered.drop(columns=next_year + '_Selected_Capacity').describe(), df_long_term_decay_summary_stats]).to_clipboard()\n",
    "# df_stage_1A_congestion_settles_pivoted.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5791206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7a542d4",
   "metadata": {},
   "source": [
    "### Zonal ARR chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb5b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zonal_arrs_base = pd.read_excel(zonal_arr_path, sheet_name='Final Results', skiprows=4).iloc[:22, 2:23].rename(\n",
    "    columns={'Unnamed: 2': 'Zone'}\n",
    ").set_index('Zone').loc[zone, :].pipe(\n",
    "    lambda series: series.where(series.index != 'PY26/27', arr_valuation_output(annual_decay_base, stage_2_dollars_per_MW_base).loc['total_dollars', next_year])\n",
    ").pipe(\n",
    "    lambda series: series.where(series.index != 'PY27/28', arr_valuation_output(annual_decay_base, stage_2_dollars_per_MW_base).loc['total_dollars', next_to_next_year])\n",
    ").pipe(\n",
    "    lambda series: series.where(series.index != 'PY28/29', arr_valuation_output(annual_decay_base, stage_2_dollars_per_MW_base).loc['total_dollars', next_to_next_to_next_year])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c1a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "zonal_arrs_good = pd.read_excel(zonal_arr_path, sheet_name='Final Results', skiprows=4).iloc[:22, 2:23].rename(\n",
    "    columns={'Unnamed: 2': 'Zone'}\n",
    ").set_index('Zone').loc[zone, :].pipe(\n",
    "    lambda series: series.where(series.index != 'PY26/27', arr_valuation_output(annual_decay_good, stage_2_dollars_per_MW_good).loc['total_dollars', next_year])\n",
    ").pipe(\n",
    "    lambda series: series.where(series.index != 'PY27/28', arr_valuation_output(annual_decay_good, stage_2_dollars_per_MW_good).loc['total_dollars', next_to_next_year])\n",
    ").pipe(\n",
    "    lambda series: series.where(series.index != 'PY28/29', arr_valuation_output(annual_decay_good, stage_2_dollars_per_MW_good).loc['total_dollars', next_to_next_to_next_year])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6642ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "zonal_arrs_bad = pd.read_excel(zonal_arr_path, sheet_name='Final Results', skiprows=4).iloc[:22, 2:23].rename(\n",
    "    columns={'Unnamed: 2': 'Zone'}\n",
    ").set_index('Zone').loc[zone, :].pipe(\n",
    "    lambda series: series.where(series.index != 'PY26/27', arr_valuation_output(annual_decay_bad, stage_2_dollars_per_MW_bad).loc['total_dollars', next_year])\n",
    ").pipe(\n",
    "    lambda series: series.where(series.index != 'PY27/28', arr_valuation_output(annual_decay_bad, stage_2_dollars_per_MW_bad).loc['total_dollars', next_to_next_year])\n",
    ").pipe(\n",
    "    lambda series: series.where(series.index != 'PY28/29', arr_valuation_output(annual_decay_bad, stage_2_dollars_per_MW_bad).loc['total_dollars', next_to_next_to_next_year])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27844852",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=zonal_arrs_base.index, y=zonal_arrs_base.values, mode='lines', name='base'\n",
    "))\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=zonal_arrs_bad.index, y=zonal_arrs_bad.values, mode='lines', name='bad'\n",
    "))\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=zonal_arrs_good.index, y=zonal_arrs_good.values, mode='lines', name='good'\n",
    "))\n",
    "\n",
    "fig.update_layout(title=f'{zone} ARR Valuation')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2ab9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save image\n",
    "\n",
    "# fig.write_image(f'{zone}_{current_planning_year}_{look_ahead_years}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8169452",
   "metadata": {},
   "source": [
    "## Sink Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfa3c5a",
   "metadata": {},
   "source": [
    "Here we compare the aggregated path dollars by using the residual aggregate, load aggregate and zone as the sink, and choose the one that leads to the highest path value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c24105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stage_1_resources_for_sink_selection = pd.concat(\n",
    "    pd.read_excel(\n",
    "    stage_1_resources_path,\n",
    "    sheet_name=None, # Reading all sheets and combining into single df\n",
    "    header=[0, 1]\n",
    ").values(),\n",
    "ignore_index=True\n",
    ").dropna(\n",
    "    how='all', axis=0 # Dropping rows with no values\n",
    ").iloc[:, [0, 1, 2, 5, 6, 10]]\n",
    "\n",
    "df_stage_1_resources_for_sink_selection.columns = ['Zone', 'Pnode ID', 'FTR Name', current_planning_year+'_Capacity MW', 'Retired', 'Rate_based']\n",
    "\n",
    "df_stage_1_resources_for_sink_selection.rename(\n",
    "    columns={'Pnode ID': 'PNODEID'}, inplace=True\n",
    ")\n",
    "\n",
    "df_stage_1_resources_for_sink_selection = df_stage_1_resources_for_sink_selection[\n",
    "    (df_stage_1_resources_for_sink_selection.Retired != 'Y')\n",
    "][(df_stage_1_resources_for_sink_selection.Rate_based != 'Rate-based')].dropna(subset='PNODEID').assign(\n",
    "    Zone=lambda DF: DF.Zone.str.strip()\n",
    ")\n",
    "\n",
    "df_stage_1_resources_for_sink_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83e26d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since PJM posted the retired resources and the QRRs for PY 26-27 we account for that below\n",
    "\n",
    "py_26_27_retired_resources, py_26_27_qrrs = pd.read_excel(\n",
    "    r'K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\ARRs\\ARR Path Selection\\2025\\Hemanth\\Data\\Resources\\2026-27\\2026-2027-stage-1-retired-and-initial-qualified-replacement-resources-by-zone.xlsx',\n",
    "    sheet_name=None,\n",
    "    skiprows=1\n",
    ").values()\n",
    "\n",
    "py_26_27_retired_resources = py_26_27_retired_resources.rename(\n",
    "    columns={\n",
    "        'Pnode ID': 'PNODEID'\n",
    "    }\n",
    ").drop(\n",
    "    columns='Historical Unit Name'\n",
    ")\n",
    "\n",
    "py_26_27_retired_resources # These resources should be removed for sink selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58145ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "py_26_27_qrrs.rename(\n",
    "    columns={\n",
    "        'Pnode ID': 'PNODEID'\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "py_26_27_qrrs[current_planning_year + '_Capacity MW'] = 1\n",
    "\n",
    "py_26_27_qrrs # These resources will be appended and given a proxy capacity of 1 MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sink_node = pd.read_excel(\n",
    "    r'K:\\Valuation\\MODELS\\VQSWAP\\VQSwap Update Info\\PJM\\ARRs\\ARR Path Selection\\2025\\Hemanth\\Data\\Sink Node ID.xlsx'\n",
    ").dropna(\n",
    "    subset=['Zone_Node', 'Resid_Agg_Node', 'Load_Agg_Node'],\n",
    "    how='all' # dropping zones with no sinks\n",
    ")\n",
    "\n",
    "df_sink_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e3e7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stage_1_resources_for_sink_selection.loc[\n",
    "    lambda DF: ~DF.PNODEID.isin(py_26_27_retired_resources.PNODEID.values) # filtering out resources slated to be retired\n",
    "].pipe(\n",
    "    lambda DF: pd.concat([DF, py_26_27_qrrs], ignore_index=True) # adding QRRs with proxy capacity of 1\n",
    ").reset_index(drop=True).merge(\n",
    "    df_sink_node,\n",
    "    on='Zone',\n",
    "    how='left'\n",
    ").merge( # Merging with source LMP\n",
    "    right=df_long_term_ftr_results[[most_recent_LT_auction_round + '_' + 'YR1'+ '_' + most_recent_LT_auction]],\n",
    "    how='left',\n",
    "    on='PNODEID',\n",
    "    validate='m:1',\n",
    ").rename(\n",
    "    columns={\n",
    "        most_recent_LT_auction_round + '_' + 'YR1'+ '_' + most_recent_LT_auction: 'Source_LMP'\n",
    "    }\n",
    ").merge( # Merging with Zone LMP\n",
    "    right=df_long_term_ftr_results[[most_recent_LT_auction_round + '_' + 'YR1'+ '_' + most_recent_LT_auction]],\n",
    "    how='left',\n",
    "    left_on='Zone_Node',\n",
    "    right_on='PNODEID',\n",
    "    validate='m:1',\n",
    ").rename(\n",
    "    columns={\n",
    "        most_recent_LT_auction_round + '_' + 'YR1'+ '_' + most_recent_LT_auction: 'Zone_Node_LMP'\n",
    "    }\n",
    ").merge( # Merging with Residual Aggregate LMP\n",
    "    right=df_long_term_ftr_results[[most_recent_LT_auction_round + '_' + 'YR1'+ '_' + most_recent_LT_auction]],\n",
    "    how='left',\n",
    "    left_on='Resid_Agg_Node',\n",
    "    right_on='PNODEID',\n",
    "    validate='m:1',\n",
    ").rename(\n",
    "    columns={\n",
    "        most_recent_LT_auction_round + '_' + 'YR1'+ '_' + most_recent_LT_auction: 'Resid_Agg_Node_LMP'\n",
    "    }\n",
    ").merge( # Merging with Load Aggregate LMP\n",
    "    right=df_long_term_ftr_results[[most_recent_LT_auction_round + '_' + 'YR1'+ '_' + most_recent_LT_auction]],\n",
    "    how='left',\n",
    "    left_on='Load_Agg_Node',\n",
    "    right_on='PNODEID',\n",
    "    validate='m:1',\n",
    ").rename(\n",
    "    columns={\n",
    "        most_recent_LT_auction_round + '_' + 'YR1'+ '_' + most_recent_LT_auction: 'Load_Agg_Node_LMP'\n",
    "    }\n",
    ").assign( # Calculates the path value for each sink and multiplying it by capacity (after adding a threshold of 0) to get total dollars\n",
    "    Zone_Node_Path_Value=lambda DF: DF.Zone_Node_LMP - DF.Source_LMP,\n",
    "    Resid_Agg_Node_Path_Value=lambda DF: DF.Resid_Agg_Node_LMP - DF.Source_LMP,\n",
    "    Load_Agg_Node_Path_Value=lambda DF: DF.Load_Agg_Node_LMP - DF.Source_LMP,\n",
    "    Zone_Node_Dollars=lambda DF: (DF.Zone_Node_Path_Value > 0) * DF.Zone_Node_Path_Value * DF[current_planning_year + '_Capacity MW'],\n",
    "    Resid_Agg_Node_Dollars=lambda DF: (DF.Resid_Agg_Node_Path_Value > 0) * DF.Resid_Agg_Node_Path_Value * DF[current_planning_year + '_Capacity MW'],\n",
    "    Load_Agg_Node_Dollars=lambda DF: (DF.Load_Agg_Node_Path_Value > 0) * DF.Load_Agg_Node_Path_Value * DF[current_planning_year + '_Capacity MW'] \n",
    ").pivot_table( # Summing up dollars based on different sinks\n",
    "    index='Sink_Zone',\n",
    "    values=['Zone_Node_Dollars', 'Resid_Agg_Node_Dollars', 'Load_Agg_Node_Dollars'],\n",
    "    aggfunc='sum'\n",
    ").loc[\n",
    "    lambda DF: (DF != 0).any(axis=1) # Dropping zones where all dollar are 0\n",
    "].assign(\n",
    "    Sink_Selection=lambda DF: DF[['Load_Agg_Node_Dollars', 'Resid_Agg_Node_Dollars', 'Zone_Node_Dollars']].idxmax(axis=1).str.rstrip('_Dollars') # Choosing the sink based on the largest\n",
    ").to_excel('ARR sink selection PY 26-27 after PJM update.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbd519a",
   "metadata": {},
   "source": [
    "## MSRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r'K:\\Valuation\\Wholesale for Retail\\_Full Requirements\\PJM\\MD-PEPCO\\2026-01\\ARRs\\msrs_reports_recon\\APS'\n",
    "\n",
    "df_arr = pd.DataFrame()\n",
    "df_nits = pd.DataFrame()\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    if 'ARR' in file_name:\n",
    "        df_arr_temp = pd.read_csv(\n",
    "            file_path,\n",
    "            skiprows=4\n",
    "        )\n",
    "\n",
    "        df_arr = pd.concat([\n",
    "            df_arr,\n",
    "            df_arr_temp\n",
    "        ])\n",
    "    \n",
    "    else:\n",
    "\n",
    "        df_nits_temp = pd.read_csv(\n",
    "            file_path,\n",
    "            skiprows=4\n",
    "        )\n",
    "\n",
    "        df_nits = pd.concat(\n",
    "            [\n",
    "                df_nits,\n",
    "                df_nits_temp\n",
    "            ]\n",
    "        )\n",
    "\n",
    "df_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87a4b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75700261",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arr[['Date', 'ARR Target Credit ($)']].groupby('Date').sum().reset_index().merge(\n",
    "    df_nits[['Date', 'Daily Peak Load (MW)']],\n",
    "    on='Date'\n",
    ").assign(\n",
    "    Zonal_peak_share=lambda DF: DF['Daily Peak Load (MW)'] / 8937.6, #6765.9,\n",
    "    Zonal_ARR_dollars=lambda DF: DF['ARR Target Credit ($)'] / DF['Zonal_peak_share']\n",
    ").to_clipboard()\n",
    "\n",
    "# .pipe(\n",
    "#     lambda DF: DF.Zonal_ARR_dollars.sum()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d25523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
